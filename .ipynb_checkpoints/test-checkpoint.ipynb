{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54eabc84-d793-4183-920d-ac4f20ab2902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jellyfish in /opt/conda/lib/python3.11/site-packages (1.2.0)\n",
      "Requirement already satisfied: faker in /opt/conda/lib/python3.11/site-packages (37.1.0)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.11/site-packages (from faker) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish\n",
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c7fa1b45-7b04-4cc2-a017-5cfbf9ffec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import jellyfish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from packages.generateDataSets import SyntheticMatcherDataset\n",
    "from packages.calculateStatistics import DatasetEvaluator\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937c68bc-bef6-402f-8add-6fbeddfb9a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: {'gt': 5, 'tp': 4, 'fp': 2, 'fn': 1}\n",
      "Ground Truth Size: 5\n",
      "True Positives: 0\n",
      "False Positives: 0\n",
      "False Negatives: 5\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "Elapsed Time: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data for df1\n",
    "data1 = [\n",
    "    ['ID00005', 'N039', 'E298', 'Q412', 'V409', 'R232'], #TP1\n",
    "    ['ID00009', 'R822', 'W179', 'H017', 'P323', 'F298'], #TP2\n",
    "    ['ID00007', 'R449', 'X716', 'M948', 'G667', 'S702'], #TP3\n",
    "    ['ID00004', 'N002', 'E396', 'N843', 'I458', 'S719'], #TP4\n",
    "    ['ID10004', 'N002', 'E396', 'N853', 'I623', 'S569'], #FN1\n",
    "    ['NEW72378', 'J547', 'B222', 'G492', 'R551', 'S490'], #FP1\n",
    "    ['ID00008', 'N322', 'K685', 'T442', 'C825', 'W967'], #FP2\n",
    "    ['ID00000', 'W815', 'L281', 'R155', 'F768', 'B914'],\n",
    "    ['ID00001', 'C172', 'B326', 'X400', 'M508', 'O776'],\n",
    "    ['ID00002', 'V683', 'C265', 'J127', 'D589', 'F482'],\n",
    "    ['ID00003', 'E851', 'P721', 'F745', 'D863', 'K229'],\n",
    "    ['ID00016', 'T873', 'D670', 'U046', 'Z181', 'X621'],\n",
    "    ['ID00017', 'F327', 'G856', 'E567', 'O929', 'Q721'],\n",
    "    ['ID00010', 'O283', 'T723', 'Z034', 'V319', 'X338'],\n",
    "]\n",
    "\n",
    "# Data for df2\n",
    "data2 = [\n",
    "    ['ID00005', 'R746', 'E298', 'Q412', 'L291', 'R232'], #TP1\n",
    "    ['ID00009', 'R822', 'W179', 'H017', 'P323', 'F298'], #TP2\n",
    "    ['ID00007', 'Z011', 'X716', 'M948', 'W967', 'S702'], #TP3\n",
    "    ['ID00004', 'N002', 'E396', 'N843', 'V935', 'S719'], #TP4\n",
    "    ['ID10004', 'N002', 'E396', 'N553', 'I453', 'S459'], #FN1\n",
    "    ['NEW80187', 'J547', 'B222', 'G492', 'W673', 'S490'], #FP1\n",
    "    ['NEW30110', 'N322', 'K685', 'T432', 'C225', 'W967'], #FP2\n",
    "    ['NEW72832', 'F875', 'Q768', 'H822', 'Z154', 'X678'], \n",
    "    ['NEW30110', 'R560', 'C434', 'M687', 'Q689', 'Q863'],\n",
    "    ['NEW81243', 'R762', 'N687', 'A109', 'K476', 'R637'],\n",
    "    ['NEW52689', 'A089', 'V733', 'W158', 'A640', 'H331'],\n",
    "    ['NEW67368', 'Z079', 'J617', 'G878', 'W111', 'Q500'],\n",
    "    ['NEW72348', 'J547', 'B222', 'G492', 'R551', 'S490'],\n",
    "    ['NEW34469', 'Y990', 'H898', 'W673', 'L967', 'M829'],\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "columns = ['id', 'col1', 'col2', 'col3', 'col4', 'col5']\n",
    "df1 = pd.DataFrame(data1, columns=columns)\n",
    "df2 = pd.DataFrame(data2, columns=columns)\n",
    "expected = {'gt': 5, 'tp': 4, 'fp': 2, 'fn': 1}\n",
    "\n",
    "evaluator = DatasetEvaluator(df1, df2, expected, threshold=3, match_column='id')\n",
    "evaluator.evaluate()\n",
    "# evaluator.calculateStatistics()\n",
    "evaluator.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16be8e8c-f259-4f3f-a3f3-a66b8be074af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1 avg time: 578.75 sec\n",
      "Pipeline 2 avg time: 114.03 sec\n",
      "-80.30% reduce time ---> improvement\n"
     ]
    }
   ],
   "source": [
    "time1 =  578.75\n",
    "time2 = 114.03\n",
    "\n",
    "print(f\"Pipeline 1 avg time: {time1} sec\")\n",
    "print(f\"Pipeline 2 avg time: { time2} sec\")\n",
    "\n",
    "if time1 < time2:\n",
    "    print(f\"{(1 - time1/time2)*100:.2f}% increase time ---> not improvement\")\n",
    "else:\n",
    "    print(f\"-{(1 - time2/time1)*100:.2f}% reduce time ---> improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d584d-8e52-4c56-ae07-eb0be03cd46b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "1250 Elapsed Time: 1.72 seconds\n",
    "2500 Elapsed Time: 7.02 seconds \n",
    "5000 Elapsed Time: 28.09 seconds\n",
    "10000 Elapsed Time: 114.03 seconds\n",
    "\n",
    "\n",
    "1250 Elapsed Time: 6.29 seconds\n",
    "2500 Elapsed Time: 26 seconds \n",
    "5000 Elapsed Time: 108.56 seconds\n",
    "10000 Elapsed Time: 578.75 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "542d83a7-fcda-424e-92c1-f666c1be7136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: {'gt': 125, 'tp': 93, 'fp': 69, 'fn': 32}\n",
      "Ground Truth Size: 125\n",
      "True Positives: 93\n",
      "False Positives: 69\n",
      "False Negatives: 32\n",
      "Precision: 0.5741\n",
      "Recall: 0.7440\n",
      "Elapsed Time: 0.33 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = SyntheticMatcherDataset(size=500 ,  ground_truth_ratio=0.25, datasets_ratio = (1, 1), true_positive_ratio=0.75, threshold=3)\n",
    "df1, df2 = dataset.df1, dataset.df2\n",
    "expected = dataset.expected\n",
    "\n",
    "evaluator = DatasetEvaluator(df1, df2, expected, threshold=3, match_column=\"id\")\n",
    "evaluator.evaluate()\n",
    "evaluator.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eb71f50e-bcb0-4081-a5f2-f59d1be187bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657210111618042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93, 32, 69, 0.5740740740740741, 0.744)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fast_chunk(s):\n",
    "    bs = s.encode('utf-8')\n",
    "    chunk_len = len(bs) // 4 * 4  # ensure multiple of 4\n",
    "    return np.frombuffer(bs[:chunk_len], dtype='S4')\n",
    "    \n",
    "# Step 1: Find best 3-column combination (least unique combinations)\n",
    "columns = ['col1', 'col2', 'col3', 'col4', 'col5']\n",
    "unique_counts = []\n",
    "\n",
    "for cols in combinations(columns, 3):\n",
    "    count = df2[list(cols)].astype(str).agg(''.join, axis=1).nunique()\n",
    "    unique_counts.append((cols, count))\n",
    "\n",
    "best_combination = min(unique_counts, key=lambda x: x[1])\n",
    "best_cols = list(best_combination[0])\n",
    "\n",
    "# Step 2: Build index based on best 3 columns\n",
    "df2['index_key'] = df2[best_cols].astype(str).agg(''.join, axis=1)\n",
    "df2_proc = df2.apply(lambda x: (x['id'], ''.join(map(str, x[1:6])), x['index_key']), axis=1).to_numpy()\n",
    "\n",
    "df1['index_key'] = df1[best_cols].astype(str).agg(''.join, axis=1)\n",
    "df1_proc = df1.apply(lambda x: (x['id'], ''.join(map(str, x[1:6])), x['index_key']), axis=1).to_numpy()\n",
    "\n",
    "\n",
    "# Step 3: Build hashed buckets from df2 (by index_key)\n",
    "hashed_bucket = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for id_val, full_str, index_key in df2_proc:\n",
    "    hashed_bucket[index_key][full_str].append(id_val)\n",
    "\n",
    "# Precompute chunked df1 strings once\n",
    "chunked_df1 = [(fast_chunk(hash), fast_chunk(combined), match_id) for match_id, combined, hash in df1_proc]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for index_key, df1_chunks, match_id in chunked_df1:\n",
    "    if key in hashed_bucket:\n",
    "        if np.count_nonzero(index_key == fast_chunk(key)) == 0:\n",
    "            continue\n",
    "\n",
    "        for full_str in hashed_bucket[key]:\n",
    "            df2_chunks = fast_chunk(full_str)\n",
    "            if len(df1_chunks) != len(df2_chunks):\n",
    "                continue\n",
    "\n",
    "            match_count = np.count_nonzero(df1_chunks == df2_chunks)\n",
    "            if match_count >= 3:\n",
    "                hashed_bucket[key][full_str].append(match_id)\n",
    "                break\n",
    "\n",
    "print(time.time() - start)\n",
    "\n",
    "flat = []\n",
    "for subdict in hashed_bucket.values():\n",
    "    for ids in subdict.values():\n",
    "        flat.append(ids)\n",
    "\n",
    "fp, tp, fn = 0, 0, 0\n",
    "\n",
    "for bucket in flat:\n",
    "    if len(bucket) > 1:\n",
    "        ids_to_check = np.array(bucket[1:])\n",
    "        if (any(np.isin(ids_to_check, ground_truth_ids_np))):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "fn = len(ground_truth_ids_np) - tp\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "tp, fn, fp, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a8311a37-eeb8-4b76-b25c-5cb013424683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082980155944824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2327, -2202, 69, 0.9712020033388982, 18.616)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def fast_chunk(s):\n",
    "    bs = s.encode('utf-8')\n",
    "    chunk_len = len(bs) // 4 * 4  # ensure multiple of 4\n",
    "    return np.frombuffer(bs[:chunk_len], dtype='S4')\n",
    "\n",
    "ground_truth_ids_np = np.intersect1d(df1['id'], df2['id'])\n",
    "# Step 1: Find best 3-column combination (least unique combinations)\n",
    "columns = ['col1', 'col2', 'col3', 'col4', 'col5']\n",
    "unique_counts = []\n",
    "\n",
    "for cols in combinations(columns, 3):\n",
    "    count = df2[list(cols)].astype(str).agg(''.join, axis=1).nunique()\n",
    "    unique_counts.append((cols, count))\n",
    "\n",
    "best_combination = min(unique_counts, key=lambda x: x[1])\n",
    "best_cols = list(best_combination[0])\n",
    "\n",
    "# Step 2: Build index based on best 3 columns\n",
    "df2['index_key'] = df2[best_cols].astype(str).agg(''.join, axis=1)\n",
    "df2_proc = df2.apply(lambda x: (x['id'], ''.join(map(str, x[1:6])), x['index_key']), axis=1).to_numpy()\n",
    "\n",
    "df1['index_key'] = df1[best_cols].astype(str).agg(''.join, axis=1)\n",
    "df1_proc = df1.apply(lambda x: (x['id'], ''.join(map(str, x[1:6])), x['index_key']), axis=1).to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Build hashed buckets from df2 (by index_key)\n",
    "hashed_bucket = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for id_val, full_str, index_key in df2_proc:\n",
    "    hashed_bucket[index_key][full_str].append(id_val)\n",
    "\n",
    "# Precompute chunked df1 strings once\n",
    "chunked_df1 = [(hash, fast_chunk(combined), match_id) for match_id, combined, hash in df1_proc]\n",
    "\n",
    "fp, fn, tp = 0, 0 , 0\n",
    "start = time.time()\n",
    "\n",
    "for index_key, df1_chunks, match_id in chunked_df1:\n",
    "    for key in list(hashed_bucket.keys()):\n",
    "        fast_key1 = fast_chunk(index_key)\n",
    "        if np.count_nonzero(fast_key1 == fast_chunk(key)) == 0:\n",
    "            continue\n",
    "\n",
    "        for full_str in hashed_bucket[key]:\n",
    "            df2_chunks = fast_chunk(full_str)\n",
    "            if len(df1_chunks) != len(df2_chunks):\n",
    "                continue\n",
    "\n",
    "            match_count = np.count_nonzero(df1_chunks == df2_chunks)\n",
    "            if match_count >= 3:\n",
    "                hashed_bucket[key][full_str].append(match_id)\n",
    "                break\n",
    "\n",
    "print(time.time() - start)\n",
    "\n",
    "flat = []\n",
    "for subdict in hashed_bucket.values():\n",
    "    for ids in subdict.values():\n",
    "        flat.append(ids)\n",
    "\n",
    "for bucket in flat:\n",
    "    if len(bucket) > 1:\n",
    "        ids_to_check = np.array(bucket[1:])\n",
    "        mask = np.isin(ids_to_check, ground_truth_ids_np)\n",
    "        matched |= set(ids_to_check[mask])\n",
    "        fp += mask.size - np.count_nonzero(mask)\n",
    "\n",
    "tp = len(matched)\n",
    "fn = len(ground_truth_ids_np) - tp\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "tp, fn, fp, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8f0b9a-2b77-4f2e-b1be-e85695360596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1 avg time: 6.8511 sec\n",
      "Pipeline 2 avg time: 5.4194 sec\n",
      "-0.21% reduce time ---> improvement\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "def pipeline1():\n",
    "    dataset = SyntheticMatcherDataset(size=1000 , true_positive_ratio=0.70, threshold=3)\n",
    "    df1, df2 = dataset.df1, dataset.df2\n",
    "    expected = dataset.expected\n",
    "    \n",
    "    unique_tokens = pd.unique(pd.concat([df1.iloc[:, 1:6], df2.iloc[:, 1:6]], axis=0).stack())\n",
    "    token_map = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "    \n",
    "    def map_row(row):\n",
    "        return [token_map[val] for val in row[1:6]]\n",
    "        \n",
    "    # Keep column 0 as-is\n",
    "    df1_ids = df1.iloc[:, [0]]\n",
    "    \n",
    "    # Apply mapping only on columns 1 to 5\n",
    "    df1_mapped = df1.iloc[:, 1:6].apply(map_row, axis=1, result_type='expand')\n",
    "    \n",
    "    # Concatenate back with column 0\n",
    "    df1_final = pd.concat([df1_ids, df1_mapped], axis=1)\n",
    "    \n",
    "    # Repeat for df2\n",
    "    df2_ids = df2.iloc[:, [0]]\n",
    "    df2_mapped = df2.iloc[:, 1:6].apply(map_row, axis=1, result_type='expand')\n",
    "    df2_final = pd.concat([df2_ids, df2_mapped], axis=1)\n",
    "        \n",
    "    evaluator = DatasetEvaluator(df1, df2, expected, threshold=3, match_column=\"id\")\n",
    "    evaluator.evaluate()\n",
    "    # evaluator.printResults()\n",
    "\n",
    "def pipeline2():\n",
    "    dataset = SyntheticMatcherDataset(size=1000 , true_positive_ratio=0.70, threshold=3)\n",
    "    df1, df2 = dataset.df1, dataset.df2\n",
    "    expected = dataset.expected\n",
    "    \n",
    "    evaluator = DatasetEvaluator(df1, df2, expected, threshold=3, match_column=\"id\")\n",
    "    evaluator.evaluate()\n",
    "    # evaluator.printResults()\n",
    "\n",
    "time1 = timeit.timeit(pipeline1, number=10)\n",
    "time2 = timeit.timeit(pipeline2, number=10)\n",
    "\n",
    "print(f\"Pipeline 1 avg time: {time1 / 10:.4f} sec\")\n",
    "print(f\"Pipeline 2 avg time: {time2 / 10:.4f} sec\")\n",
    "\n",
    "if time1 < time2:\n",
    "    print(f\"{1 - time1/time2:.2f}% increase time ---> not improvement\")\n",
    "else:\n",
    "    print(f\"-{1 - time2/time1:.2f}% reduce time ---> improvement\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f50b75-71d4-4d0b-ad56-936d7e1e988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1746562494.410388"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline 1 avg time: 6.2479 sec\n",
    "Pipeline 2 avg time: 6.0408 sec\n",
    "\n",
    "Expected: {'gt': 125, 'tp': 87, 'fp': 38, 'fn': 38}\n",
    "Ground Truth Size: 125\n",
    "True Positives: 87\n",
    "False Positives: 38\n",
    "False Negatives: 38\n",
    "Precision: 0.6960\n",
    "Recall: 0.6960\n",
    "Elapsed Time: 1.26 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe4ae274-62b8-463b-aea5-0fe0c0caaa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03% reduce time ---> improvement\n"
     ]
    }
   ],
   "source": [
    "if time1 < time2:\n",
    "    print(f\"{1 - time1/time2:.2f}% increase time ---> not improvement\")\n",
    "else:\n",
    "    print(f\"-{1 - time2/time1:.2f}% reduce time ---> improvement\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6ae61f4-f66b-4a16-9bf5-5653b61a1894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9668528625618208"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47325ccd-a77c-4341-aba0-4e83c56cd822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R746E298Q412L291R232': ['ID00005'],\n",
       " 'R822W179H017P323F298': ['ID00009'],\n",
       " 'Z011X716M948W967S702': ['ID00007'],\n",
       " 'N002E396N843V935S719': ['ID00004'],\n",
       " 'N002E396N553I453S459': ['ID10004']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'R746E298Q412L291R232': ['ID00005'], 'R822W179H017P323F298': ['ID00009'], 'Z011X716M948W967S702': ['ID00007'], 'N002E396N843V935S719': ['ID00004'], 'N002E396N553I453S459': ['ID10004']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "feeba8ad-8cee-4741-8298-705d4ca5a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best column triplet (least unique values): ('col1', 'col2', 'col3')\n",
      "Number of unique combinations: 10000\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "columns = ['col1', 'col2', 'col3', 'col4', 'col5']\n",
    "\n",
    "# Store result as (column_triplet, unique_count)\n",
    "unique_counts = []\n",
    "\n",
    "for cols in combinations(columns, 3):\n",
    "    count = df2[list(cols)].agg(''.join, axis=1).nunique()\n",
    "    unique_counts.append((cols, count))\n",
    "\n",
    "# Find the combination with the minimum unique count\n",
    "best_combination = min(unique_counts, key=lambda x: x[1])\n",
    "\n",
    "print(\"Best column triplet (least unique values):\", best_combination[0])\n",
    "print(\"Number of unique combinations:\", best_combination[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "955f3877-0ca7-4987-a52d-2ce10c5c47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_proc = df1.apply(lambda x: (x[0], ''.join(map(str, x[1:]))), axis=1).to_numpy()\n",
    "df2_proc = df2.apply(lambda x: (x[0], ''.join(map(str, x[1:]))), axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d26648ad-cf14-48a8-9bb0-be6fcabdfebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, ['col1', 'col2', 'col3'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd1 = df2[['col1', 'col2', 'col3']].agg(''.join, axis=1).nunique(), ['col1', 'col2', 'col3']\n",
    "asd2 = df2[['col2', 'col3', 'col4']].agg(''.join, axis=1).nunique(), ['col2', 'col3', 'col4']\n",
    "asd3 = df2[['col3', 'col4', 'col5']].agg(''.join, axis=1).nunique(), ['col3', 'col4', 'col5']\n",
    "min([asd1,asd2,asd3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713762a3-d40f-4cbc-b253-8d3134514f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_chunk(s):\n",
    "    bs = s.encode('utf-8')\n",
    "    chunk_len = len(bs) // 4 * 4  # ensure multiple of 4\n",
    "    return np.frombuffer(bs[:chunk_len], dtype='S4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab94a920-46c4-46e9-9164-0cf8859722ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'F553', b'F553', b'F553', b'F553', b'F553'], dtype='|S4')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f74c437-e401-44af-b055-c4eb6c0f6943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('col1', 'col2', 'col3'), 10000),\n",
       " (('col1', 'col2', 'col4'), 10000),\n",
       " (('col1', 'col2', 'col5'), 10000),\n",
       " (('col1', 'col3', 'col4'), 10000),\n",
       " (('col1', 'col3', 'col5'), 10000),\n",
       " (('col1', 'col4', 'col5'), 10000),\n",
       " (('col2', 'col3', 'col4'), 10000),\n",
       " (('col2', 'col3', 'col5'), 10000),\n",
       " (('col2', 'col4', 'col5'), 10000),\n",
       " (('col3', 'col4', 'col5'), 10000)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219fbf04-6da4-4c30-a07f-ddbbfec93e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3191a21-96d1-4a3d-aa9f-faa0c59e7d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "140bc717-3140-492b-9472-6b67f3bc3098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 3, 0.5714285714285714, 0.6666666666666666)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "51a47082-13b9-456e-96fc-83b26884f86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 6, 0, 0, 0.0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate precision/recall\n",
    "matched = set()\n",
    "fp = 0\n",
    "ground_truth_ids_np = np.intersect1d(df1['id'], df2['id'])\n",
    "\n",
    "for bucket in hashed_bucket.values():\n",
    "    if len(bucket) > 1:\n",
    "        ids_to_check = np.array(bucket[1:])  # exclude the original\n",
    "        mask = np.isin(ids_to_check, ground_truth_ids_np)\n",
    "        matched |= set(ids_to_check[mask])\n",
    "        fp += mask.size - np.count_nonzero(mask)\n",
    "\n",
    "tp = len(matched)\n",
    "fn = len(ground_truth_ids_np) - tp\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "tp, fn, fp, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "06064040-d6bd-4989-ad7e-dcea2fe08e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ID00001', 'ID00009', 'ID00011', 'ID00013', 'ID00014', 'ID00016'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(np.intersect1d(df1['id'], df2['id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6570ebe1-c1c4-41b6-bf0e-505580ed8bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(np.intersect1d(df1['id'], df2['id']))).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46c7c310-7f5e-43d2-9edf-2a2f79a19e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ID00017'], dtype='<U7')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b528c-a2aa-471d-a64c-ae75c2ef3b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
