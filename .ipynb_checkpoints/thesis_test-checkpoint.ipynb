{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54eabc84-d793-4183-920d-ac4f20ab2902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jellyfish in /opt/conda/lib/python3.11/site-packages (1.1.3)\n",
      "Requirement already satisfied: faker in /opt/conda/lib/python3.11/site-packages (37.1.0)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.11/site-packages (from faker) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish\n",
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fa1b45-7b04-4cc2-a017-5cfbf9ffec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import jellyfish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "import math\n",
    "from itertools import combinations, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263d1dfd-42da-463a-9eed-86cf9556e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, df1: pd.DataFrame, \n",
    "                 df2: pd.DataFrame, \n",
    "                 matchColumn: str, \n",
    "                 on: List = [],\n",
    "                 method: str = 'column', \n",
    "                 threshold: float  = 0.6):\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "        self.on = on\n",
    "        self.threshold = threshold\n",
    "\n",
    "        if method not in [\"concat\", \"column\"]:\n",
    "            raise ValueError(f\"Method '{method}' is not correct.\")\n",
    "        self.method = method\n",
    "\n",
    "    \n",
    "        if matchColumn not in self.df1.columns or matchColumn not in self.df2.columns:\n",
    "            raise ValueError(f\"Column '{matchColumn}' is not found in both DataFrames.\")\n",
    "        self.matchColumn = matchColumn\n",
    "        \n",
    "        self.groundTruth = None\n",
    "        self.totalMatches = None        \n",
    "    \n",
    "    def setGroundTruth(self):\n",
    "        \"\"\"Sets the ground truth based on matching 'id' columns.\"\"\"\n",
    "        self.groundTruth = np.intersect1d(self.df1[self.matchColumn], self.df2[self.matchColumn])\n",
    "\n",
    "    def soundexDfs(self):\n",
    "        \"\"\"Apply soundex transformation to non-id columns.\"\"\"\n",
    "        for df in [self.df1, self.df2]:\n",
    "            for col_name in df.columns:\n",
    "                if col_name != self.matchColumn:\n",
    "                    df[col_name] = df[col_name].apply(lambda x: jellyfish.soundex(str(x)))\n",
    "\n",
    "            if self.method == 'concat':\n",
    "                non_match_columns = [col for col in df.columns if col != self.matchColumn]\n",
    "                df['concatenated'] = df[non_match_columns].apply(lambda row: ''.join(row.astype(str)), axis=1)\n",
    "                df.drop(columns=non_match_columns, inplace=True)\n",
    "\n",
    "    def setTotalMatches(self):\n",
    "        \"\"\"Sets the total matches based on merged DataFrames.\"\"\"\n",
    "        \n",
    "        # if self.method == 'concat':\n",
    "        #     self.totalMatches = self.df1.merge(self.df2, how=\"inner\", on=['concatenated']).to_numpy()\n",
    "        # else:   \n",
    "        #     self.totalMatches = self.df1.merge(self.df2, how=\"outer\", on=self.on + [self.matchColumn]).to_numpy()\n",
    "\n",
    "        self.totalMatches =  self.df1.merge(pd.concat([self.df1, self.df2]), how='outer', on=self.on)[[\"0_y\"] + self.on]\n",
    "        self.totalMatches.rename(columns={'0_y': self.matchColumn}, inplace=True)\n",
    "        \n",
    "    def printStatistics(self):\n",
    "        \"\"\"Print statistics (True Positives, False Positives, Precision).\"\"\"\n",
    "        myStatistics = self.Statistics(groundTruth=self.groundTruth, \n",
    "                                       totalMatches=self.totalMatches, \n",
    "                                       threshold=self.threshold, \n",
    "                                       on=self.on, \n",
    "                                       matchColumn=self.matchColumn)\n",
    "        myStatistics.calculate()\n",
    "\n",
    "    # Inner class Statistics\n",
    "    class Statistics:\n",
    "        def __init__(self,\n",
    "                     groundTruth: pd.DataFrame, \n",
    "                     totalMatches: pd.DataFrame, \n",
    "                     threshold : float = 0.8,\n",
    "                     matchColumn: str | int = 1,\n",
    "                     on: List =[]):\n",
    "            self.groundTruth = pd.DataFrame(groundTruth)\n",
    "            self.totalMatches = pd.DataFrame(totalMatches)\n",
    "            self.threshold = threshold\n",
    "            self.matchColumn = matchColumn\n",
    "            self.on = on\n",
    "\n",
    "            self._setThresholdValues()\n",
    "            \n",
    "        def calculate(self):\n",
    "            # self.result = self.totalMatches.groupby(self.matchColumn)\\\n",
    "            #         .filter(lambda x : len(x) >=2)\\\n",
    "            #         .groupby(self.matchColumn)\\\n",
    "            #         .apply(lambda x: x.iloc[:, 1:].apply(lambda x: x.nunique() == 1)).sum(axis=1)\n",
    "\n",
    "            duplicates = self.totalMatches[self.totalMatches[[1,2,3,4,5]].duplicated(keep=False)].sort_values(by=[1,2,3,4,5])\n",
    "        \n",
    "            # Function to check if two rows match at least 3/5 columns\n",
    "            def is_duplicate(row1, row2):\n",
    "                return sum(row1 == row2) >=  self.matchingRows  # At least 3 matches out of 5\n",
    "\n",
    "            # print(duplicates)\n",
    "            \n",
    "            duplicate_pairs = []\n",
    "            # Find duplicates\n",
    "            for i in range(len(duplicates)):\n",
    "                for j in range(i + 1, len(duplicates)):  # Compare only unique pairs\n",
    "                    if is_duplicate(duplicates.iloc[i, 1:], duplicates.iloc[j, 1:]):\n",
    "                        duplicate_pairs.append((i, j, duplicates.iloc[i, 0] == duplicates.iloc[j, 0]))  # Store (index1, index2, same_id)\n",
    "\n",
    "            similar_rows = []\n",
    "            for i, j in combinations(range(self.totalMatches(df)), 2):  # Unique row pairs\n",
    "                if is_similar(df.iloc[i, 1:], df.iloc[j, 1:]):  # Compare columns 1-5\n",
    "                    similar_rows.append((i, j))\n",
    "        \n",
    "            print(duplicate_pairs)\n",
    "            # Count same ID and different ID duplicates\n",
    "            tp = sum(1 for _, _, same_id in duplicate_pairs if same_id)\n",
    "            fp = len(duplicate_pairs) - same_id_count\n",
    "            fn = self.groundTruth.size - tp\n",
    "            \n",
    "            precision = tp / (tp + fp) if tp + fp != 0 else 0 \n",
    "            recall = tp / (tp + fn)  if tp + fn != 0 else 0\n",
    "            f1_score = (2 * precision * recall) / (precision + recall)\n",
    "            \n",
    "            print(\"Total Possible Mathces:\", self.groundTruth.size)\n",
    "            print(\"True Positives (TP):\", tp)\n",
    "            print(\"False Positives (FP):\", fp)\n",
    "            print(\"False Negatives (FN):\", fn)\n",
    "            print(\"Precision:\", f\"{precision:.4f}\")\n",
    "            print(\"Recall:\", f\"{recall:.4f}\")\n",
    "            print(\"F1-score:\", f\"{f1_score:.4f}\")\n",
    "\n",
    "        def _matchingAlgorithm(self, group):\n",
    "            return group.nunique() == 1\n",
    "            \n",
    "        def _setThresholdValues(self) -> List:\n",
    "            size = len(self.totalMatches.columns) - 1\n",
    "            limit = math.floor(self.threshold * size)\n",
    "            \n",
    "            print(f\"We accept at least {limit}/{size} as matches!\") \n",
    "            self.matchingRows = limit\n",
    "            # return [i for i in range(size, limit , -1)]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603541e9-4f75-435c-86aa-7e45489ef7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create two datasets with slight variations\n",
    "# Data have 3 matches and one \n",
    "data1 = {\n",
    "    0: [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    1: [\"Kostas\", \"Maria\", \"John\", \"Sophia\", \"George\", \"Eleni\", \"Michael\", \"Anna\", \"Chris\", \"Dimitris\"],\n",
    "    2: [\"Razgkelis\", \"Papadopoulos\", \"Smith\", \"Johnson\", \"Pavlou\", \"Nikolaou\", \"Brown\", \"Miller\", \"Taylor\", \"Andreas\"],\n",
    "    3: [\"Orestiada\", \"Thessaloniki\", \"Grevena\", \"Athina\", \"Aleksandroupoli\", \"Giannena\", \"Larissa\", \"Komotini\", \"Trikala\", \"Kozani\"],\n",
    "    4: ['Jennifer Lights', 'Brandon Lakes', 'Aguilar Stravenue', 'Richardson Ferry', 'Freeman Way', \n",
    "        'Gabrielle Underpass', 'Burns Summit', 'Heather Village', 'Jamie Common', 'Greg Lock'],\n",
    "    5:  ['Cooper and Sons', 'Pope LLC', 'Fowler-Smith', 'Torres PLC', 'Jones LLC', 'White, Duncan and Robinson', 'Hayden Inc', \n",
    "         'Wilson and Sons', 'Peterson, Smith and Robinson','Hudson, Phelps and Day'],\n",
    "    \n",
    "}\n",
    "\n",
    "data2 = {\n",
    "    0: [101, 202, 203, 204, 205, 206, 207, 208, 209, 110],\n",
    "    1: [\"Kistas\", \"Maria\", \"John\", \"Sophasdia\", \"Giorge\", \"Elendsi\", \"Micheal\", \"Ana\", \"Khris\", \"Dimtris\"],\n",
    "    2: [\"Rozgkliiis\", \"Papadopoulos\", \"Smith\", \"Johnson\", \"Pavlodvu\", \"Nikolaou\", \"Batrrroun\", \"antMiler\", \"Tttayloor\", \"Andres\"],\n",
    "    3: [\"Orestiada\", \"Thessaloniki\", \"Grevena\", \"Athina\", \"Aleksandrouasdpoli\", \"Gianasdna\", \"Larasdissa\", \"Komasdini\", \"Trsadala\", \"Koxani\"],\n",
    "    4: [\"Jnnfer Lights\", 'Brandon Lakes', 'Aguilar Stravenue', 'RichardasasdaFerry', 'Freasdeman Way', 'Gabrielle Underpass', 'Burasdas mmit', 'Heatasasdllage', 'JamasdCommon', 'Grg Lck'],\n",
    "    5:  ['Cpeeer and ons', 'Pope LLC', 'Fowler-Smith', 'Torvasd PLC', 'Jonasda LLC', 'Whitasddvuncan and Robinson', 'Hayasdasv Inc', \n",
    "         'Wasdvand Sons', 'Petersosdvaith and Robinson','Htsn, Phelps and Day'],\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6b22753-5166-46e7-b4d2-b0875bc3e1e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We accept at least 2/5 as matches!\n",
      "      0     1     2     3     4     5\n",
      "12  110  D536  A536  K250  G624  H325\n",
      "13  110  D536  A536  K250  G624  H325\n",
      "4   103  J500  S530  G615  A246  F462\n",
      "5   203  J500  S530  G615  A246  F462\n",
      "0   101  K232  R242  O623  J516  C165\n",
      "1   101  K232  R242  O623  J516  C165\n",
      "2   102  M600  P131  T245  B653  P142\n",
      "3   202  M600  P131  T245  B653  P142\n",
      "[(0, 1, True), (2, 3, False), (4, 5, True), (6, 7, False)]\n",
      "Total Possible Mathces: 2\n",
      "True Positives (TP): 2\n",
      "False Positives (FP): 2\n",
      "False Negatives (FN): 0\n",
      "Precision: 0.5000\n",
      "Recall: 1.0000\n",
      "F1-score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to DataFrame\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Run pipeline and see statistics\n",
    "pipeline = MyClass(df1, df2, matchColumn=0, on=[1, 2, 3, 4, 5], threshold=0.4)\n",
    "pipeline.setGroundTruth()\n",
    "pipeline.soundexDfs()\n",
    "pipeline.setTotalMatches()\n",
    "pipeline.printStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b590ab60-2d2c-431e-b8a2-81132567745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    101\n",
      "Name: 0, dtype: int64\n",
      "0    110\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ed744a7-fed0-42a1-9375-1a97798bb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = pipeline.totalMatches[pipeline.totalMatches[[1,2,3,4,5]].duplicated(keep=False)].sort_values(by=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c57472b1-df39-49fe-a292-e626c608b884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.groupby([1, 2, 3, 4, 5])[0].nunique().eq(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf96e60-4c71-44b2-a2e3-a0e1db018055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.groupby([1, 2, 3, 4, 5])[0].nunique().gt(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d2014b-6f08-4937-8171-ca33dd0e6936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH  =  \"data/\"\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(PATH, 'df1.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "df2 = pd.read_csv(os.path.join(PATH, 'df5.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "\n",
    "# Run pipeline and see statistics\n",
    "pipeline = MyClass(df1, df2, matchColumn=0, on=[1,2,3,4,5], method=\"column\", threshold = 0.6) #  --> this means at least 3/5 of the fields must match \n",
    "pipeline.setGroundTruth()\n",
    "pipeline.soundexDfs()\n",
    "# pipeline.setTotalMatches()\n",
    "# pipeline.printStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "161fecff-2c32-4c03-9922-19a0fe2eb141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding similar rows:   0%|          | 182161/20058542778 [01:27<2688:20:12, 2072.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_103/1220678964.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtotal_comparisons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# Total comparisons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_comparisons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Finding similar rows\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unique row pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Compare columns 1-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0msimilar_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Store matched row indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;31m#  _getitem_lowerdim is only called after a check for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3655\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m             result = self._constructor_sliced(new_mgr, name=self.index[i]).__finalize__(\n\u001b[1;32m   3657\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3658\u001b[0m             )\n\u001b[0;32m-> 3659\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3660\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3662\u001b[0m         \u001b[0;31m# icol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, ref, copy)\u001b[0m\n\u001b[0;32m-> 4153\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def is_similar(row1, row2, threshold=4):\n",
    "    return sum(row1 == row2) >= threshold\n",
    "\n",
    "# Find similar rows with progress bar\n",
    "similar_rows = []\n",
    "total_comparisons = df.shape[0] * (df.shape[0] - 1) // 2  # Total comparisons\n",
    "\n",
    "with tqdm(total=total_comparisons, desc=\"Finding similar rows\") as pbar:\n",
    "    for i, j in combinations(range(df.shape[0]), 2):  # Unique row pairs\n",
    "        if is_similar(df.iloc[i, 1:], df.iloc[j, 1:]):  # Compare columns 1-5\n",
    "            similar_rows.append((i, j))  # Store matched row indices\n",
    "        pbar.update(1)  # Update progress bar\n",
    "\n",
    "# Display results\n",
    "print(f\"Found {len(similar_rows)} similar row pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5e3fb-00e7-462d-b1ec-e880e896b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pipeline.totalMatches)\n",
    "\n",
    "# Convert to NumPy array for faster computation\n",
    "data = df.iloc[:, 1:].to_numpy()  # Exclude ID column\n",
    "\n",
    "# Compute similarity matrix (all row comparisons)\n",
    "similar_matrix = np.equal(data[:, None, :], data[None, :, :]).sum(axis=2)\n",
    "\n",
    "# Find indices where at least 4 out of 5 values match (excluding self-comparisons)\n",
    "similar_pairs = np.argwhere((similar_matrix >= 4) & (np.triu(np.ones(similar_matrix.shape), k=1) == 1))\n",
    "\n",
    "# Convert result to a list of tuples (row index pairs)\n",
    "similar_rows = [tuple(pair) for pair in similar_pairs]\n",
    "\n",
    "# Display results\n",
    "print(f\"Found {len(similar_rows)} similar row pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b88128d-97ed-4832-8ef1-f1ad997eba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13544 similar row pairs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "data = {\n",
    "    0: [110, 110, 103, 203, 101, 101, 102, 202],  # ID column\n",
    "    1: ['D536', 'D536', 'J500', 'J500', 'K232', 'K232', 'M600', 'M600'],\n",
    "    2: ['A536', 'A536', 'S530', 'S530', 'R242', 'R232', 'P131', 'P131'],\n",
    "    3: ['K250', 'K250', 'G615', 'G615', 'O623', 'O623', 'T245', 'T245'],\n",
    "    4: ['G624', 'G624', 'A246', 'A246', 'J516', 'J516', 'B653', 'B653'],\n",
    "    5: ['H325', 'H325', 'F462', 'F462', 'C165', 'C135', 'P142', 'P142'],\n",
    "}\n",
    "\n",
    "# Convert totalMatches to DataFrame\n",
    "#df = pd.DataFrame(data)\n",
    "df = pd.DataFrame(pipeline.totalMatches)\n",
    "data = df.iloc[:, 1:].to_numpy()  # Exclude ID column\n",
    "\n",
    "# Function to compare a row against all others\n",
    "def find_similar_pairs(i):\n",
    "    matches = (np.equal(data[i], data).sum(axis=1) >= 4)  # Compare row i with all\n",
    "    return [(i, j) for j in np.where(matches)[0] if j > i]  # Store pairs (i, j)\n",
    "\n",
    "# Parallel execution\n",
    "num_jobs = -1  # Use all available CPU cores\n",
    "similar_rows = Parallel(n_jobs=num_jobs)(delayed(find_similar_pairs)(i) for i in range(len(data)))\n",
    "\n",
    "# Flatten the list\n",
    "similar_rows = [pair for sublist in similar_rows for pair in sublist]\n",
    "\n",
    "# Display results\n",
    "print(f\"Found {len(similar_rows)} similar row pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eaf96010-fe71-4730-bc53-478f358cb408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6665682220909628, 0.36112, 9028, 4516, 15972)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = 0  # False Positives\n",
    "tp = 0  # True Positives\n",
    "\n",
    "# Convert ground truth to a set for faster lookup\n",
    "ground_truth_set = pipeline.groundTruth\n",
    "\n",
    "# Iterate through similar row pairs\n",
    "for i, j in similar_rows:\n",
    "    row_i = pipeline.totalMatches.iloc[i][0]  # Convert row to tuple\n",
    "    row_j = pipeline.totalMatches.iloc[j][0]  \n",
    "\n",
    "    if row_i in ground_truth_set and row_j in ground_truth_set:  # Check if either row is in ground truth\n",
    "        tp += 1  # True Positive\n",
    "\n",
    "\n",
    "# Avoid division by zero\n",
    "fp = len(similar_rows) - tp\n",
    "fn = 25_000 - tp\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "precision, recall, tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19019988-dbc3-44f6-84d2-e01d5e88fdca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA100000</td>\n",
       "      <td>B323</td>\n",
       "      <td>J520</td>\n",
       "      <td>A620</td>\n",
       "      <td>3523</td>\n",
       "      <td>G650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA100004</td>\n",
       "      <td>B620</td>\n",
       "      <td>M240</td>\n",
       "      <td>E421</td>\n",
       "      <td>2251</td>\n",
       "      <td>B645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA100004</td>\n",
       "      <td>B620</td>\n",
       "      <td>M240</td>\n",
       "      <td>E421</td>\n",
       "      <td>2251</td>\n",
       "      <td>B645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA100006</td>\n",
       "      <td>B620</td>\n",
       "      <td>D520</td>\n",
       "      <td>W460</td>\n",
       "      <td>3345</td>\n",
       "      <td>H616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA100007</td>\n",
       "      <td>K260</td>\n",
       "      <td>K600</td>\n",
       "      <td>L500</td>\n",
       "      <td>1225</td>\n",
       "      <td>G650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200288</th>\n",
       "      <td>AL262640</td>\n",
       "      <td>F655</td>\n",
       "      <td>E540</td>\n",
       "      <td>L200</td>\n",
       "      <td>3223</td>\n",
       "      <td>M624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200289</th>\n",
       "      <td>AL262641</td>\n",
       "      <td>T500</td>\n",
       "      <td>G625</td>\n",
       "      <td>A425</td>\n",
       "      <td>1626</td>\n",
       "      <td>A214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200290</th>\n",
       "      <td>AL262642</td>\n",
       "      <td>E430</td>\n",
       "      <td>C645</td>\n",
       "      <td>N240</td>\n",
       "      <td>1214</td>\n",
       "      <td>F432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200291</th>\n",
       "      <td>AL262643</td>\n",
       "      <td>S645</td>\n",
       "      <td>J650</td>\n",
       "      <td>W450</td>\n",
       "      <td>1263</td>\n",
       "      <td>A214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200292</th>\n",
       "      <td>AL262644</td>\n",
       "      <td>H560</td>\n",
       "      <td>C460</td>\n",
       "      <td>E421</td>\n",
       "      <td>7623</td>\n",
       "      <td>W455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200293 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0     1     2     3     4     5\n",
       "0       AA100000  B323  J520  A620  3523  G650\n",
       "1       AA100004  B620  M240  E421  2251  B645\n",
       "2       AA100004  B620  M240  E421  2251  B645\n",
       "3       AA100006  B620  D520  W460  3345  H616\n",
       "4       AA100007  K260  K600  L500  1225  G650\n",
       "...          ...   ...   ...   ...   ...   ...\n",
       "200288  AL262640  F655  E540  L200  3223  M624\n",
       "200289  AL262641  T500  G625  A425  1626  A214\n",
       "200290  AL262642  E430  C645  N240  1214  F432\n",
       "200291  AL262643  S645  J650  W450  1263  A214\n",
       "200292  AL262644  H560  C460  E421  7623  W455\n",
       "\n",
       "[200293 rows x 6 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.totalMatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a992d-74d7-4b3e-91f4-55cab029ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.6665682220909628, 0.36112, 9028, 4516, 15972)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24669c-7a00-4d46-ab8e-568e915cd32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(row1, row2, threshold=4):\n",
    "    return np.sum(row1[1:] == row2[1:]) >= threshold  # Compare columns 1-5\n",
    "\n",
    "# df1 = pd.DataFrame({\n",
    "#     0: [101, 102, 103, 104],\n",
    "#     1: ['X123', 'Y456', 'Z789', 'X123'],\n",
    "#     2: ['K250', 'G624', 'J500', 'S530'],\n",
    "#     3: ['R242', 'P131', 'T245', 'M600'],\n",
    "#     4: ['O623', 'B653', 'G615', 'A246'],\n",
    "#     5: ['J516', 'P142', 'F462', 'C165']\n",
    "# }).to_numpy()\n",
    "\n",
    "# df2 = pd.DataFrame({\n",
    "#     0: [101, 102, 203, 204],\n",
    "#     1: ['X123', 'Z789', 'Y456', 'M999'],\n",
    "#     2: ['K250', 'G624', 'S530', 'T111'],\n",
    "#     3: ['R242', 'P131', 'T245', 'M610'],\n",
    "#     4: ['O623', 'B653', 'G615', 'A256'],\n",
    "#     5: ['J516', 'P142', 'F462', 'D999']\n",
    "# }).to_numpy()\n",
    "\n",
    "# df1 = pd.concat([pipeline.df1.sample(n=750, random_state=9), pd.DataFrame(pipeline.groundTruth[:250, ]).merge(pipeline.df1, on=[0])]).to_numpy()\n",
    "# df2 = pd.concat([pipeline.df2.sample(n=750, random_state=10), pd.DataFrame(pipeline.groundTruth[:250, ]).merge(pipeline.df2, on=[0])]).to_numpy()\n",
    "\n",
    "df1 = pipeline.df1.to_numpy()\n",
    "df2 = pipeline.df2.to_numpy()\n",
    "\n",
    "# ========================================================= # \n",
    "ground_truth = np.intersect1d(df1[:, 0], df2[:, 0])\n",
    "\n",
    "\n",
    "# Create all row index pairs\n",
    "row_pairs = list(product(df1[:, 0], df2[:, 0]))\n",
    "print(row_pairs)\n",
    "\n",
    "# Compare rows and store similar ones\n",
    "totalMatches = [(id1, id2) for id1, id2 in row_pairs \n",
    "                 if is_similar(df1[df1[:, 0] == id1][0], df2[df2[:, 0] == id2][0])]\n",
    "\n",
    "\n",
    "tp = sum(1 for x, y in totalMatches if x == y and x in ground_truth)\n",
    "fp = len(totalMatches) - tp\n",
    "fn = len(ground_truth) - tp\n",
    "\n",
    "tp, fp ,fn, tp/(tp+fp), tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f61193-7dfa-4df8-aa13-db3078038007",
   "metadata": {},
   "outputs": [],
   "source": [
    "3, (169, 13, 83, 0.9285714285714286, 0.6706349206349206)\n",
    "4, (83, 0, 169, 1.0, 0.32936507936507936)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434913f3-5136-4702-83b2-efcd0c1ea41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process chunks\n",
    "def process_chunk(chunk_row_pairs, df1, df2, ground_truth):\n",
    "    totalMatches = []\n",
    "    for id1, id2 in chunk_row_pairs:\n",
    "        if is_similar(df1[df1[:, 0] == id1][0], df2[df2[:, 0] == id2][0]):\n",
    "            totalMatches.append((id1, id2))\n",
    "\n",
    "    # Calculate tp, fp, fn for this chunk\n",
    "    tp = sum(1 for x, y in totalMatches if x == y and x in ground_truth)\n",
    "    fp = len(totalMatches) - tp\n",
    "    fn = len(ground_truth) - tp\n",
    "    \n",
    "    return tp, fp, fn\n",
    "    \n",
    "df1 = pipeline.df1.to_numpy()\n",
    "df2 = pipeline.df2.to_numpy()\n",
    "# Split row pairs into chunks\n",
    "\n",
    "chunk_size = 100  # Adjust chunk size based on memory and performance\n",
    "row_pairs = list(product(df1[:, 0], df2[:, 0]))\n",
    "print(\"done\")\n",
    "# Process in chunks\n",
    "total_tp, total_fp, total_fn = 0, 0, 0\n",
    "for i in range(0, len(row_pairs), chunk_size):\n",
    "    chunk_row_pairs = row_pairs[i:i + chunk_size]\n",
    "    tp, fp, fn = process_chunk(chunk_row_pairs, df1, df2, ground_truth)\n",
    "    \n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "    break\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0\n",
    "\n",
    "print(f\"True Positives: {total_tp}\")\n",
    "print(f\"False Positives: {total_fp}\")\n",
    "print(f\"False Negatives: {total_fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff153e9-8134-4db8-bddf-edac31f7f3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
