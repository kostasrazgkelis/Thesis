{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54eabc84-d793-4183-920d-ac4f20ab2902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jellyfish in /opt/conda/lib/python3.11/site-packages (1.2.0)\n",
      "Requirement already satisfied: faker in /opt/conda/lib/python3.11/site-packages (37.1.0)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.11/site-packages (from faker) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish\n",
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fa1b45-7b04-4cc2-a017-5cfbf9ffec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import jellyfish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "import math\n",
    "from itertools import combinations, product\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263d1dfd-42da-463a-9eed-86cf9556e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, df1: pd.DataFrame, \n",
    "                 df2: pd.DataFrame, \n",
    "                 matchColumn: str, \n",
    "                 on: List = [],\n",
    "                 method: str = 'column', \n",
    "                 threshold: float  = 0.6):\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "        self.on = on\n",
    "        self.threshold = threshold\n",
    "\n",
    "        if method not in [\"concat\", \"column\"]:\n",
    "            raise ValueError(f\"Method '{method}' is not correct.\")\n",
    "        self.method = method\n",
    "\n",
    "    \n",
    "        if matchColumn not in self.df1.columns or matchColumn not in self.df2.columns:\n",
    "            raise ValueError(f\"Column '{matchColumn}' is not found in both DataFrames.\")\n",
    "        self.matchColumn = matchColumn\n",
    "        \n",
    "        self.groundTruth = None\n",
    "        self.totalMatches = None        \n",
    "    \n",
    "    def setGroundTruth(self):\n",
    "        \"\"\"Sets the ground truth based on matching 'id' columns.\"\"\"\n",
    "        self.groundTruth = np.intersect1d(self.df1[self.matchColumn], self.df2[self.matchColumn])\n",
    "\n",
    "    def soundexDfs(self):\n",
    "        \"\"\"Apply soundex transformation to non-id columns.\"\"\"\n",
    "        for df in [self.df1, self.df2]:\n",
    "            for col_name in df.columns:\n",
    "                if col_name != self.matchColumn:\n",
    "                    df[col_name] = df[col_name].apply(lambda x: jellyfish.soundex(str(x)))\n",
    "\n",
    "            if self.method == 'concat':\n",
    "                non_match_columns = [col for col in df.columns if col != self.matchColumn]\n",
    "                df['concatenated'] = df[non_match_columns].apply(lambda row: ''.join(row.astype(str)), axis=1)\n",
    "                df.drop(columns=non_match_columns, inplace=True)\n",
    "\n",
    "    def setTotalMatches(self):\n",
    "        \"\"\"Sets the total matches based on merged DataFrames.\"\"\"\n",
    "        \n",
    "        # if self.method == 'concat':\n",
    "        #     self.totalMatches = self.df1.merge(self.df2, how=\"inner\", on=['concatenated']).to_numpy()\n",
    "        # else:   \n",
    "        #     self.totalMatches = self.df1.merge(self.df2, how=\"outer\", on=self.on + [self.matchColumn]).to_numpy()\n",
    "\n",
    "        self.totalMatches =  self.df1.merge(pd.concat([self.df1, self.df2]), how='outer', on=self.on)[[\"0_y\"] + self.on]\n",
    "        self.totalMatches.rename(columns={'0_y': self.matchColumn}, inplace=True)\n",
    "        \n",
    "    def printStatistics(self):\n",
    "        \"\"\"Print statistics (True Positives, False Positives, Precision).\"\"\"\n",
    "        myStatistics = self.Statistics(groundTruth=self.groundTruth, \n",
    "                                       totalMatches=self.totalMatches, \n",
    "                                       threshold=self.threshold, \n",
    "                                       on=self.on, \n",
    "                                       matchColumn=self.matchColumn)\n",
    "        myStatistics.calculate()\n",
    "\n",
    "    # Inner class Statistics\n",
    "    class Statistics:\n",
    "        def __init__(self,\n",
    "                     groundTruth: pd.DataFrame, \n",
    "                     totalMatches: pd.DataFrame, \n",
    "                     threshold : float = 0.8,\n",
    "                     matchColumn: str | int = 1,\n",
    "                     on: List =[]):\n",
    "            self.groundTruth = pd.DataFrame(groundTruth)\n",
    "            self.totalMatches = pd.DataFrame(totalMatches)\n",
    "            self.threshold = threshold\n",
    "            self.matchColumn = matchColumn\n",
    "            self.on = on\n",
    "\n",
    "            self._setThresholdValues()\n",
    "            \n",
    "        def calculate(self):\n",
    "            # self.result = self.totalMatches.groupby(self.matchColumn)\\\n",
    "            #         .filter(lambda x : len(x) >=2)\\\n",
    "            #         .groupby(self.matchColumn)\\\n",
    "            #         .apply(lambda x: x.iloc[:, 1:].apply(lambda x: x.nunique() == 1)).sum(axis=1)\n",
    "\n",
    "            duplicates = self.totalMatches[self.totalMatches[[1,2,3,4,5]].duplicated(keep=False)].sort_values(by=[1,2,3,4,5])\n",
    "        \n",
    "            # Function to check if two rows match at least 3/5 columns\n",
    "            def is_duplicate(row1, row2):\n",
    "                return sum(row1 == row2) >=  self.matchingRows  # At least 3 matches out of 5\n",
    "\n",
    "            # print(duplicates)\n",
    "            \n",
    "            duplicate_pairs = []\n",
    "            # Find duplicates\n",
    "            for i in range(len(duplicates)):\n",
    "                for j in range(i + 1, len(duplicates)):  # Compare only unique pairs\n",
    "                    if is_duplicate(duplicates.iloc[i, 1:], duplicates.iloc[j, 1:]):\n",
    "                        duplicate_pairs.append((i, j, duplicates.iloc[i, 0] == duplicates.iloc[j, 0]))  # Store (index1, index2, same_id)\n",
    "\n",
    "            # Count same ID and different ID duplicates\n",
    "            tp = sum(1 for _, _, same_id in duplicate_pairs if same_id)\n",
    "            fp = len(duplicate_pairs) - same_id_count\n",
    "            fn = self.groundTruth.size - tp\n",
    "            \n",
    "            precision = tp / (tp + fp) if tp + fp != 0 else 0 \n",
    "            recall = tp / (tp + fn)  if tp + fn != 0 else 0\n",
    "            f1_score = (2 * precision * recall) / (precision + recall)\n",
    "            \n",
    "            print(\"Total Possible Mathces:\", self.groundTruth.size)\n",
    "            print(\"True Positives (TP):\", tp)\n",
    "            print(\"False Positives (FP):\", fp)\n",
    "            print(\"False Negatives (FN):\", fn)\n",
    "            print(\"Precision:\", f\"{precision:.4f}\")\n",
    "            print(\"Recall:\", f\"{recall:.4f}\")\n",
    "            print(\"F1-score:\", f\"{f1_score:.4f}\")\n",
    "\n",
    "        def _matchingAlgorithm(self, group):\n",
    "            return group.nunique() == 1\n",
    "            \n",
    "        def _setThresholdValues(self) -> List:\n",
    "            size = len(self.totalMatches.columns) - 1\n",
    "            limit = math.floor(self.threshold * size)\n",
    "            \n",
    "            print(f\"We accept at least {limit}/{size} as matches!\") \n",
    "            self.matchingRows = limit\n",
    "            # return [i for i in range(size, limit , -1)]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "603541e9-4f75-435c-86aa-7e45489ef7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create two datasets with slight variations\n",
    "# Data have 3 matches and one \n",
    "data1 = {\n",
    "    0: [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    1: [\"Kostas\", \"Maria\", \"John\", \"Sophia\", \"George\", \"Eleni\", \"Michael\", \"Anna\", \"Chris\", \"Dimitris\"],\n",
    "    2: [\"Razgkelis\", \"Papadopoulos\", \"Smith\", \"Johnson\", \"Pavlou\", \"Nikolaou\", \"Brown\", \"Miller\", \"Taylor\", \"Andreas\"],\n",
    "    3: [\"Orestiada\", \"Thessaloniki\", \"Grevena\", \"Athina\", \"Aleksandroupoli\", \"Giannena\", \"Larissa\", \"Komotini\", \"Trikala\", \"Kozani\"],\n",
    "    4: ['Jennifer Lights', 'Brandon Lakes', 'Aguilar Stravenue', 'Richardson Ferry', 'Freeman Way', \n",
    "        'Gabrielle Underpass', 'Burns Summit', 'Heather Village', 'Jamie Common', 'Greg Lock'],\n",
    "    5:  ['Cooper and Sons', 'Pope LLC', 'Fowler-Smith', 'Torres PLC', 'Jones LLC', 'White, Duncan and Robinson', 'Hayden Inc', \n",
    "         'Wilson and Sons', 'Peterson, Smith and Robinson','Hudson, Phelps and Day'],\n",
    "    \n",
    "}\n",
    "\n",
    "data2 = {\n",
    "    0: [101, 202, 203, 204, 205, 206, 207, 208, 209, 110],\n",
    "    1: [\"Kistas\", \"Maria\", \"John\", \"Sophasdia\", \"Giorge\", \"Elendsi\", \"Micheal\", \"Ana\", \"Khris\", \"Dimtris\"],\n",
    "    2: [\"Rozgkliiis\", \"Papadopoulos\", \"Smith\", \"Johnson\", \"Pavlodvu\", \"Nikolaou\", \"Batrrroun\", \"antMiler\", \"Tttayloor\", \"Andres\"],\n",
    "    3: [\"Orestiada\", \"Thessaloniki\", \"Grevena\", \"Athina\", \"Aleksandrouasdpoli\", \"Gianasdna\", \"Larasdissa\", \"Komasdini\", \"Trsadala\", \"Koxani\"],\n",
    "    4: [\"Jnnfer Lights\", 'Brandon Lakes', 'Aguilar Stravenue', 'RichardasasdaFerry', 'Freasdeman Way', 'Gabrielle Underpass', 'Burasdas mmit', 'Heatasasdllage', 'JamasdCommon', 'Grg Lck'],\n",
    "    5:  ['Cpeeer and ons', 'Pope LLC', 'Fowler-Smith', 'Torvasd PLC', 'Jonasda LLC', 'Whitasddvuncan and Robinson', 'Hayasdasv Inc', \n",
    "         'Wasdvand Sons', 'Petersosdvaith and Robinson','Htsn, Phelps and Day'],\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d6b22753-5166-46e7-b4d2-b0875bc3e1e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrame\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Run pipeline and see statistics\n",
    "pipeline = MyClass(df1, df2, matchColumn=0, on=[1, 2, 3, 4, 5], threshold=0.4)\n",
    "pipeline.setGroundTruth()\n",
    "pipeline.soundexDfs()\n",
    "pipeline.setTotalMatches()\n",
    "# pipeline.printStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b590ab60-2d2c-431e-b8a2-81132567745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    101\n",
      "Name: 0, dtype: int64\n",
      "0    110\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b7d2014b-6f08-4937-8171-ca33dd0e6936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH  =  \"data/\"\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(PATH, 'df1.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "df2 = pd.read_csv(os.path.join(PATH, 'df5.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "\n",
    "# Run pipeline and see statistics\n",
    "pipeline = MyClass(df1, df2, matchColumn=0, on=[1,2,3,4,5], method=\"column\", threshold = 0.6) #  --> this means at least 3/5 of the fields must match \n",
    "pipeline.setGroundTruth()\n",
    "pipeline.soundexDfs()\n",
    "# pipeline.setTotalMatches()\n",
    "# pipeline.printStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "434913f3-5136-4702-83b2-efcd0c1ea41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 133\n",
      "False Positives: 5\n",
      "False Negatives: 67\n",
      "Precision: 0.9638\n",
      "Recall: 0.6650\n",
      "Elapsed Time: 52.30 seconds\n"
     ]
    }
   ],
   "source": [
    "def is_similar(row1, row2, threshold=4):\n",
    "    return np.sum(row1[1:] == row2[1:]) >= threshold  # Compare columns 1-5\n",
    "\n",
    "\n",
    "# Function to process chunks\n",
    "def process_chunk(chunk_row_pairs, df1, df2, ground_truth):\n",
    "    totalMatches = []\n",
    "    for id1, id2 in chunk_row_pairs:\n",
    "        if is_similar(df1[df1[:, 0] == id1][0], df2[df2[:, 0] == id2][0], 3):\n",
    "            totalMatches.append((id1, id2))\n",
    "\n",
    "    # Calculate tp, fp, fn for this chunk\n",
    "    tp = sum(1 for x, y in totalMatches if x == y and x in ground_truth)\n",
    "    fp = len(totalMatches) - tp\n",
    "    fn = len(ground_truth) - tp\n",
    "    \n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "size = 200\n",
    "\n",
    "df1 = pd.concat([pipeline.df1.iloc[:size], pipeline.df1.iloc[25_000:25000 + size * 3]]).to_numpy()\n",
    "df2 = pd.concat([pipeline.df2.iloc[:size], pipeline.df2.iloc[25_000:25000 + size * 3]]).to_numpy()\n",
    "\n",
    "ground_truth = np.intersect1d(df1[:, 0], df2[:, 0])\n",
    "\n",
    "# Split row pairs into chunks\n",
    "chunk_size = 1000  # Adjust chunk size based on memory and performance\n",
    "\n",
    "# Process in chunks\n",
    "total_tp, total_fp, total_fn = 0, 0, 0\n",
    "for i in range(0, len(df1), chunk_size):\n",
    "    # Generate row pairs for the current chunk\n",
    "    chunk_row_pairs = list(product(df1[i:i + chunk_size, 0], df2[:, 0]))\n",
    "    tp, fp, fn = process_chunk(chunk_row_pairs, df1, df2, ground_truth)\n",
    "    \n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"True Positives: {total_tp}\")\n",
    "print(f\"False Positives: {total_fp}\")\n",
    "print(f\"False Negatives: {total_fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ff153e9-8134-4db8-bddf-edac31f7f3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pipeline.df1.iloc[:250], pipeline.df1.iloc[25_000:25000 + 750]]).to_numpy().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14c5636f-3a0d-46cb-97c1-3691cfe54de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1745433922.1039279"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([7.85, 8.35, 9.89, 8.78, 8.87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "370fd24f-2174-4c55-b124-ad7db2765a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 330\n",
      "False Positives: 50\n",
      "False Negatives: 170\n",
      "Precision: 0.8684\n",
      "Recall: 0.6600\n",
      "Elapsed Time: 26.34 seconds\n"
     ]
    }
   ],
   "source": [
    "def is_similar(row1, row2, threshold=3):\n",
    "    return np.sum(row1[1:] == row2[1:]) >= threshold\n",
    "\n",
    "def process_chunk(chunk_row_pairs, df1_dict, df2_dict, ground_truth, threshold=3):\n",
    "    total_matches = []\n",
    "    for id1, id2 in chunk_row_pairs:\n",
    "        row1 = df1_dict.get(id1)\n",
    "        row2 = df2_dict.get(id2)\n",
    "        if row1 is not None and row2 is not None:\n",
    "            if is_similar(row1, row2, threshold):\n",
    "                total_matches.append((id1, id2))\n",
    "\n",
    "    tp = sum(1 for id1, id2 in total_matches if id1 == id2 and id1 in ground_truth)\n",
    "    fp = len(total_matches) - tp\n",
    "    \n",
    "    return tp, fp\n",
    "    \n",
    "# Preprocess\n",
    "size = 500\n",
    "df1 = pd.concat([pipeline.df1.iloc[:size], pipeline.df1.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "df2 = pd.concat([pipeline.df2.iloc[:size], pipeline.df2.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "\n",
    "df1_dict = {row[0]: row for row in df1}\n",
    "df2_dict = {row[0]: row for row in df2}\n",
    "ground_truth = set(np.intersect1d(df1[:, 0], df2[:, 0]))\n",
    "\n",
    "chunk_size = 500\n",
    "total_tp = total_fp = total_fn = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(df1), chunk_size):\n",
    "    chunk_ids = df1[i:i + chunk_size, 0]\n",
    "    chunk_row_pairs = list(product(chunk_ids, df2[:, 0]))\n",
    "    tp, fp = process_chunk(chunk_row_pairs, df1_dict, df2_dict, ground_truth)\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "\n",
    "total_fn = len(ground_truth) - total_tp\n",
    "\n",
    "precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "\n",
    "print(f\"True Positives: {total_tp}\")\n",
    "print(f\"False Positives: {total_fp}\")\n",
    "print(f\"False Negatives: {total_fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8b05e-b4b7-4e3e-8b95-a8195a080432",
   "metadata": {},
   "outputs": [],
   "source": [
    "148.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e2e5719-6aa9-4c05-b735-648c0af65c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7da9b7-4e7b-4d18-931e-121cc76efd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "\n",
    "True Positives: 3437\n",
    "False Positives: 1697\n",
    "False Negatives: 1563\n",
    "Precision: 0.6695\n",
    "Recall: 0.6874\n",
    "Elapsed Time: 2599.92 seconds\n",
    "\n",
    "n = 15000\n",
    "\n",
    "True Positives: 9903\n",
    "False Positives: 8710\n",
    "False Negatives: 5097\n",
    "Precision: 0.5320\n",
    "Recall: 0.6602\n",
    "Elapsed Time: 26709.04 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "36185f65-6e33-40ad-a33f-d6a3f658570c",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[333], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# df1 = pd.concat([pipeline.df1.iloc[:size], pipeline.df1.iloc[25000:25000 + size * 3]]).to_numpy()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# df2 = pd.concat([pipeline.df2.iloc[:size], pipeline.df2.iloc[25000:25000 + size * 3]]).to_numpy()\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m ground_truth_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(np\u001b[38;5;241m.\u001b[39mintersect1d(\u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, df2[:,\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     19\u001b[0m df2_buckets \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df2:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3660\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3660\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:5737\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5735\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5736\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5737\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "# Index df2 by a hash on a few features to reduce comparisons\n",
    "# For example, use a subset of columns to pre-group similar rows\n",
    "# Here, we use the first column (ID) as a rough bucket\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def is_similar(row1, row2, threshold=2):\n",
    "    return np.sum(row1[1:] == row2[1:]) >= threshold\n",
    "\n",
    "# Preprocess\n",
    "size = 500\n",
    "df1 = pd.concat([pipeline.df1.iloc[:size], pipeline.df1.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "df2 = pd.concat([pipeline.df2.iloc[:size], pipeline.df2.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "\n",
    "ground_truth_ids = set(np.intersect1d(df1[:,0], df2[:,0]))\n",
    "\n",
    "df2_buckets = defaultdict(list)\n",
    "for row in df2:\n",
    "    df2_buckets[row[0]].append(row)  # You can change the key to a composite or substring\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "matched = set()\n",
    "\n",
    "for row1 in df1:\n",
    "    candidates = df2_buckets.get(row1[0], [])\n",
    "    for row2 in candidates:\n",
    "        if is_similar(row1, row2, threshold=3):\n",
    "            matched.add((row1[0], row2[0]))  # Track ID pairs\n",
    "            break\n",
    "\n",
    "total_tp = sum(1 for id1, id2 in matched if id1 == id2 and id1 in ground_truth_ids)\n",
    "total_fp = sum(1 for id1, id2 in matched if id1 != id2 or id1 not in ground_truth_ids)\n",
    "total_fn = len(ground_truth_ids) - total_tp \n",
    "\n",
    "precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"True Positives: {total_tp}\")\n",
    "print(f\"False Positives: {total_fp}\")\n",
    "print(f\"False Negatives: {total_fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d22d65-0dea-4312-9fa9-0d363028687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e61a9ab-5dfa-42af-930f-5322c13c0b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AA100000', 'AA100004', 'AA100006', ..., 'AB16959', 'AB16960',\n",
       "       'AB16962'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c91134fc-82b6-4b71-98bf-44c1f8e61488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "3252896a-d1e7-406d-b68c-f047e685eebc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[362], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# df1 = pd.concat([df1[:size], df1[25000:25000 + size * 3]])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# df2 = pd.concat([df2[:size], df2[25000:25000 + size * 3]])\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     21\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m0\u001b[39m] ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, x[\u001b[38;5;241m1\u001b[39m:]))), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     23\u001b[0m id_df1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([row[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df1])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[362], line 20\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# df1 = pd.concat([df1[:size], df1[25000:25000 + size * 3]])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# df2 = pd.concat([df2[:size], df2[25000:25000 + size * 3]])\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m df1 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m0\u001b[39m] ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, x[\u001b[38;5;241m1\u001b[39m:]))), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     21\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m0\u001b[39m] ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, x[\u001b[38;5;241m1\u001b[39m:]))), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     23\u001b[0m id_df1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([row[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df1])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/base.py:767\u001b[0m, in \u001b[0;36mIndexOpsMixin.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    765\u001b[0m to_list \u001b[38;5;241m=\u001b[39m tolist\n\u001b[0;32m--> 767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m    768\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m    Return an iterator of the values.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m    iterator\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# We are explicitly making element iterators.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def is_similar(row1, row2, threshold=3):\n",
    "    return np.sum(np.array(row1) == np.array(row2)) >= threshold\n",
    "\n",
    "# PATH  =  \"data/\"\n",
    "\n",
    "# df1 = pd.read_csv(os.path.join(PATH, 'df1.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "# df2 = pd.read_csv(os.path.join(PATH, 'df5.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "\n",
    "# # Run pipeline and see statistics\n",
    "# pipeline = MyClass(df1, df2, matchColumn=0, on=[1,2,3,4,5], method=\"column\", threshold = 0.6) #  --> this means at least 3/5 of the fields must match \n",
    "# pipeline.setGroundTruth()\n",
    "# pipeline.soundexDfs()\n",
    "\n",
    "\n",
    "# size = 1000\n",
    "\n",
    "# df1 = pd.concat([df1[:size], df1[25000:25000 + size * 3]])\n",
    "# df2 = pd.concat([df2[:size], df2[25000:25000 + size * 3]])\n",
    "\n",
    "df1 = df1.apply(lambda x: (x[0] ,''.join(map(str, x[1:]))), axis=1).to_numpy()\n",
    "df2 = df2.apply(lambda x: (x[0] ,''.join(map(str, x[1:]))), axis=1).to_numpy()\n",
    "\n",
    "id_df1 = np.array([row[0] for row in df1])\n",
    "id_df2 = np.array([row[0] for row in df2])\n",
    "\n",
    "ground_truth_ids = np.intersect1d(id_df1, id_df2)\n",
    "\n",
    "df2_buckets = defaultdict(list)\n",
    "for row in df2:\n",
    "    df2_buckets[row[1]].append(row[0])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for value, key in df1:\n",
    "    row1 = tuple(key[i:i+4] for i in range(0, len(key), 4))\n",
    "\n",
    "    for data in df2_buckets:\n",
    "        row2 = tuple(data[i:i+4] for i in range(0, len(data), 4))        \n",
    "        if is_similar(row1, row2, 3):\n",
    "            df2_buckets[data].append(value)  \n",
    "            break\n",
    "            \n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "\n",
    "matched = set()\n",
    "fp = 0\n",
    "\n",
    "for row in df2_buckets:\n",
    "    bucket = df2_buckets.get(row, [])\n",
    "\n",
    "    if len(bucket) > 1:\n",
    "        for id_ in set(bucket):\n",
    "            if id_ in ground_truth_ids:\n",
    "                matched.add(id_)\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "tp = sum(1 for x in matched if x in ground_truth_ids)\n",
    "fn = len(ground_truth_ids) - tp\n",
    "\n",
    "precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979a355-3c33-4efd-b7cb-be684c3d311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "True Positives: 330\n",
    "False Positives: 50\n",
    "False Negatives: 170\n",
    "Precision: 0.8684\n",
    "Recall: 0.6600\n",
    "Elapsed Time: 26.34 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "19aac8c3-90ed-4f46-9b80-d41a26ed3165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW69170</td>\n",
       "      <td>Z631</td>\n",
       "      <td>O057</td>\n",
       "      <td>E811</td>\n",
       "      <td>N890</td>\n",
       "      <td>V745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEW42857</td>\n",
       "      <td>E710</td>\n",
       "      <td>E567</td>\n",
       "      <td>W738</td>\n",
       "      <td>P812</td>\n",
       "      <td>P543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEW45364</td>\n",
       "      <td>G339</td>\n",
       "      <td>I476</td>\n",
       "      <td>R716</td>\n",
       "      <td>Q734</td>\n",
       "      <td>I953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW41107</td>\n",
       "      <td>T308</td>\n",
       "      <td>U581</td>\n",
       "      <td>E472</td>\n",
       "      <td>I891</td>\n",
       "      <td>R545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00900</td>\n",
       "      <td>F666</td>\n",
       "      <td>S903</td>\n",
       "      <td>W195</td>\n",
       "      <td>C685</td>\n",
       "      <td>N842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>ID00579</td>\n",
       "      <td>A833</td>\n",
       "      <td>I881</td>\n",
       "      <td>X216</td>\n",
       "      <td>R850</td>\n",
       "      <td>P441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>NEW13679</td>\n",
       "      <td>F611</td>\n",
       "      <td>Z437</td>\n",
       "      <td>W343</td>\n",
       "      <td>X130</td>\n",
       "      <td>A533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>NEW16936</td>\n",
       "      <td>F577</td>\n",
       "      <td>R193</td>\n",
       "      <td>K928</td>\n",
       "      <td>O417</td>\n",
       "      <td>S138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>NEW93404</td>\n",
       "      <td>O081</td>\n",
       "      <td>X493</td>\n",
       "      <td>C222</td>\n",
       "      <td>V901</td>\n",
       "      <td>C981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>ID00007</td>\n",
       "      <td>W131</td>\n",
       "      <td>G104</td>\n",
       "      <td>U686</td>\n",
       "      <td>G294</td>\n",
       "      <td>K920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  col1  col2  col3  col4  col5\n",
       "0    NEW69170  Z631  O057  E811  N890  V745\n",
       "1    NEW42857  E710  E567  W738  P812  P543\n",
       "2    NEW45364  G339  I476  R716  Q734  I953\n",
       "3    NEW41107  T308  U581  E472  I891  R545\n",
       "4     ID00900  F666  S903  W195  C685  N842\n",
       "..        ...   ...   ...   ...   ...   ...\n",
       "995   ID00579  A833  I881  X216  R850  P441\n",
       "996  NEW13679  F611  Z437  W343  X130  A533\n",
       "997  NEW16936  F577  R193  K928  O417  S138\n",
       "998  NEW93404  O081  X493  C222  V901  C981\n",
       "999   ID00007  W131  G104  U686  G294  K920\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e6b0d22d-6a4b-4d14-a831-0ad367959ca0",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[361], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m df1_dict \u001b[38;5;241m=\u001b[39m {row[\u001b[38;5;241m0\u001b[39m]: row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df1}\n\u001b[1;32m     24\u001b[0m df2_dict \u001b[38;5;241m=\u001b[39m {row[\u001b[38;5;241m0\u001b[39m]: row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df2}\n\u001b[0;32m---> 25\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(np\u001b[38;5;241m.\u001b[39mintersect1d(\u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, df2[:, \u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     27\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     28\u001b[0m total_tp \u001b[38;5;241m=\u001b[39m total_fp \u001b[38;5;241m=\u001b[39m total_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3660\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3660\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:5737\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5735\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5736\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5737\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "def is_similar(row1, row2, threshold=3):\n",
    "    return np.sum(row1[1:] == row2[1:]) >= threshold\n",
    "\n",
    "def process_chunk(chunk_row_pairs, df1_dict, df2_dict, ground_truth, threshold=3):\n",
    "    total_matches = []\n",
    "    for id1, id2 in chunk_row_pairs:\n",
    "        row1 = df1_dict.get(id1)\n",
    "        row2 = df2_dict.get(id2)\n",
    "        if row1 is not None and row2 is not None:\n",
    "            if is_similar(row1, row2, threshold):\n",
    "                total_matches.append((id1, id2))\n",
    "\n",
    "    tp = sum(1 for id1, id2 in total_matches if id1 == id2 and id1 in ground_truth)\n",
    "    fp = len(total_matches) - tp\n",
    "    \n",
    "    return tp, fp\n",
    "    \n",
    "# Preprocess\n",
    "size = 100\n",
    "# df1 = pd.concat([pipeline.df1.iloc[:size], pipeline.df1.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "# df2 = pd.concat([pipeline.df2.iloc[:size], pipeline.df2.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "\n",
    "df1_dict = {row[0]: row for row in df1}\n",
    "df2_dict = {row[0]: row for row in df2}\n",
    "ground_truth = set(np.intersect1d(df1[:, 0], df2[:, 0]))\n",
    "\n",
    "chunk_size = 500\n",
    "total_tp = total_fp = total_fn = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(df1), chunk_size):\n",
    "    chunk_ids = df1[i:i + chunk_size, 0]\n",
    "    chunk_row_pairs = list(product(chunk_ids, df2[:, 0]))\n",
    "    tp, fp = process_chunk(chunk_row_pairs, df1_dict, df2_dict, ground_truth)\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "\n",
    "total_fn = len(ground_truth) - total_tp\n",
    "\n",
    "precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "\n",
    "print(f\"True Positives: {total_tp}\")\n",
    "print(f\"False Positives: {total_fp}\")\n",
    "print(f\"False Negatives: {total_fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0109d9c1-33b5-402d-9ca1-4c5d8b4a39c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected True Matches  : 2\n",
      "Expected False Negatives: 1\n",
      "Actual Matching IDs     : 3 / 10 (30.00%)\n",
      "df1 shape: (10, 6), df2 shape: (10, 6)\n",
      "        id  col1  col2  col3  col4  col5\n",
      "0  ID00000  W737  O083  L781  U807  N750\n",
      "1  ID00001  M203  N237  K327  H388  E033\n",
      "2  ID00002  D293  X070  F503  N259  Z907\n",
      "3  ID00003  Q838  H798  L724  D046  V178\n",
      "4  ID00004  C905  P332  I506  R882  X144\n",
      "5  ID00005  I101  S497  X475  J147  E626\n",
      "6  ID00006  O656  F322  Q652  W307  P146\n",
      "7  ID00007  L865  S662  Q602  C806  M518\n",
      "8  ID00008  Y277  C751  G014  O671  F017\n",
      "9  ID00009  S498  C664  R896  F864  N665\n",
      "         id  col1  col2  col3  col4  col5\n",
      "0  NEW88641  L092  P218  O812  X312  G985\n",
      "1  NEW43709  R050  M302  I676  K509  B771\n",
      "2  NEW46619  W314  D085  D943  U371  U807\n",
      "3  NEW14239  R772  F547  O593  E888  H875\n",
      "4  NEW44149  B484  N080  S323  L901  Q864\n",
      "5  NEW18412  J462  P631  F149  C803  B834\n",
      "6  NEW73466  M203  N237  B551  H388  M671\n",
      "7   ID00009  S498  L278  R896  A974  N665\n",
      "8   ID00005  M226  L650  Y159  U660  P399\n",
      "9   ID00006  Z193  F322  Q652  U919  P146\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5bc7241-7063-4f40-adc9-612f5f0b35f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15 15 10 25\n",
      "Expected True Matches  : 25\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SyntheticMatcherDataset' object has no attribute 'false_positives'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m false_negatives \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_false_negatives()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Show stats\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# See samples\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(df1)\n",
      "File \u001b[0;32m~/work/packages/generateDataSets.py:124\u001b[0m, in \u001b[0;36mSyntheticMatcherDataset.stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m matching_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected True Matches  : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mground_truth_matches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected False Positves: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfalse_positives\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected False Negatives: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfalse_negatives)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual Matching IDs     : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(matching_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(matching_ids)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SyntheticMatcherDataset' object has no attribute 'false_positives'"
     ]
    }
   ],
   "source": [
    "def is_similar(row1, row2, threshold=3):\n",
    "    return np.sum(row1[1:] == row2[1:]) >= threshold\n",
    "\n",
    "from packages.generateDataSets import SyntheticMatcherDataset\n",
    "\n",
    "dataset = SyntheticMatcherDataset(size=100, match_ratio=0.25, false_positive_ratio=0.15, false_negative_ratio=0.10, threshold=3)\n",
    "\n",
    "# Access data\n",
    "df1, df2 = dataset.get_dataframes()\n",
    "ground_truth = dataset.get_ground_truth()\n",
    "false_negatives = dataset.get_false_negatives()\n",
    "\n",
    "# Show stats\n",
    "dataset.stats()\n",
    "\n",
    "# See samples\n",
    "print(df1)\n",
    "print(df2)\n",
    "\n",
    "df1 = df1.apply(lambda x: (x[0] ,''.join(map(str, x[1:]))), axis=1).to_numpy()\n",
    "df2 = df2.apply(lambda x: (x[0] ,''.join(map(str, x[1:]))), axis=1).to_numpy()\n",
    "\n",
    "id_df1 = np.array([row[0] for row in df1])\n",
    "id_df2 = np.array([row[0] for row in df2])\n",
    "\n",
    "ground_truth_ids = np.intersect1d(id_df1, id_df2)\n",
    "\n",
    "df2_buckets = defaultdict(list)\n",
    "for row in df2:\n",
    "    df2_buckets[row[1]].append(row[0])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for value, key in df1:\n",
    "    row1 = tuple(key[i:i + 4] for i in range(0, len(key), 4))\n",
    "\n",
    "    for data in df2_buckets:\n",
    "        row2 = tuple(data[i:i+4] for i in range(0, len(data), 4))        \n",
    "        if is_similar(row1, row2, 3):\n",
    "            df2_buckets[data].append(value)  \n",
    "            break\n",
    "            \n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "\n",
    "matched = set()\n",
    "fp = 0\n",
    "\n",
    "for row in df2_buckets:\n",
    "    bucket = df2_buckets.get(row, [])\n",
    "\n",
    "    if len(bucket) > 1:\n",
    "        for id_ in set(bucket):\n",
    "            if id_ in ground_truth_ids:\n",
    "                matched.add(id_)\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "tp = sum(1 for x in matched if x in ground_truth_ids)\n",
    "fn = len(ground_truth_ids) - tp\n",
    "\n",
    "precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978ae4e2-c4c9-4931-aa53-6c553d79ca86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('ID00000', 'Z133G643A830G186O673'),\n",
       "       ('ID00001', 'G502L756G011A005B975'),\n",
       "       ('ID00002', 'R918K658T053L029T391'),\n",
       "       ('ID00003', 'K087J532O236K885X553'),\n",
       "       ('ID00004', 'W468L599S556R260Q984'),\n",
       "       ('ID00005', 'H759T398I774Y355J200'),\n",
       "       ('ID00006', 'F234G707J068Z108R795'),\n",
       "       ('ID00007', 'P511U416E038U038O839'),\n",
       "       ('ID00008', 'U430U878O634Z158H676'),\n",
       "       ('ID00009', 'X922J947T824H971O196'),\n",
       "       ('ID00010', 'A983Z087B822T631U879'),\n",
       "       ('ID00011', 'W911Y522J464B848C439'),\n",
       "       ('ID00012', 'J110L844K601H338B439'),\n",
       "       ('ID00013', 'O754B943R158C714M019'),\n",
       "       ('ID00014', 'U926R329F415L384I766'),\n",
       "       ('ID00015', 'E177L593O595B274T521'),\n",
       "       ('ID00016', 'Q711R242D421A134V409'),\n",
       "       ('ID00017', 'T832U842O425G080O585'),\n",
       "       ('ID00018', 'Q575A014E340A908Y478'),\n",
       "       ('ID00019', 'B845S177B740I018G926'),\n",
       "       ('ID00020', 'Q895W534K072U530P074'),\n",
       "       ('ID00021', 'R954Z921U060P396I317'),\n",
       "       ('ID00022', 'U560Q119Z861D636O892'),\n",
       "       ('ID00023', 'W034E502B624N729R337'),\n",
       "       ('ID00024', 'J738N471S643K163N907'),\n",
       "       ('ID00025', 'Y440A732X675F756D354'),\n",
       "       ('ID00026', 'L519G951R857R700Z317'),\n",
       "       ('ID00027', 'Z087W959X757U537Q083'),\n",
       "       ('ID00028', 'E073O074M922C504E276'),\n",
       "       ('ID00029', 'W091O580U697W949N611'),\n",
       "       ('ID00030', 'G967I885I413Q038L731'),\n",
       "       ('ID00031', 'J715S879R109D256B261'),\n",
       "       ('ID00032', 'A332A593N536A662Y613'),\n",
       "       ('ID00033', 'C548G377O172Y204E160'),\n",
       "       ('ID00034', 'S766A043N144Y188H108'),\n",
       "       ('ID00035', 'Q503I431R748P596B307'),\n",
       "       ('ID00036', 'G290W758O710M842V738'),\n",
       "       ('ID00037', 'H981A529A964Q396L512'),\n",
       "       ('ID00038', 'O678L784S025P219T206'),\n",
       "       ('ID00039', 'L664S848I716V629M968'),\n",
       "       ('ID00040', 'C232J935D174D153W001'),\n",
       "       ('ID00041', 'C012A496P950F517E483'),\n",
       "       ('ID00042', 'M788N486X844H656L515'),\n",
       "       ('ID00043', 'Y161I810G907H646Z670'),\n",
       "       ('ID00044', 'P174W019I203Z349P614'),\n",
       "       ('ID00045', 'J150E994C411T847S540'),\n",
       "       ('ID00046', 'X550H067T306J686Z731'),\n",
       "       ('ID00047', 'N410L683Z726R852R119'),\n",
       "       ('ID00048', 'T054I725Y246N905M099'),\n",
       "       ('ID00049', 'V665H491E547H027N534'),\n",
       "       ('ID00050', 'N382K750C304V674V893'),\n",
       "       ('ID00051', 'M126N949C989W580K375'),\n",
       "       ('ID00052', 'T953P487M452G680F813'),\n",
       "       ('ID00053', 'H350P833Q257Y289M932'),\n",
       "       ('ID00054', 'M223Z862E298D937I531'),\n",
       "       ('ID00055', 'S138C876P076X505X813'),\n",
       "       ('ID00056', 'R864R834Y318P992T073'),\n",
       "       ('ID00057', 'K687K147L559P723A725'),\n",
       "       ('ID00058', 'G607Y768Y397I108F055'),\n",
       "       ('ID00059', 'P502P508G197N231V443'),\n",
       "       ('ID00060', 'D956O587G084G149Y818'),\n",
       "       ('ID00061', 'H615Z959T027X745A247'),\n",
       "       ('ID00062', 'S954E147J161R073B201'),\n",
       "       ('ID00063', 'R037X281U294H703Y208'),\n",
       "       ('ID00064', 'A456I182W911G038A284'),\n",
       "       ('ID00065', 'D665O332J663X042O523'),\n",
       "       ('ID00066', 'I365Z689R280D661E195'),\n",
       "       ('ID00067', 'J191I266G986P955V055'),\n",
       "       ('ID00068', 'L424S097X335W121M137'),\n",
       "       ('ID00069', 'V152E955U217R802B680'),\n",
       "       ('ID00070', 'V445Q164B106S303E269'),\n",
       "       ('ID00071', 'T481I225D417S582O066'),\n",
       "       ('ID00072', 'L844S707W326U085K727'),\n",
       "       ('ID00073', 'F977N946K757C108D081'),\n",
       "       ('ID00074', 'F026G416C635Z997J766'),\n",
       "       ('ID00075', 'K301P697X667Q415E784'),\n",
       "       ('ID00076', 'G579A113S227V578R608'),\n",
       "       ('ID00077', 'A060Y621W632A048Q342'),\n",
       "       ('ID00078', 'Z514K611U856C068E894'),\n",
       "       ('ID00079', 'J421U332Q975A785G306'),\n",
       "       ('ID00080', 'H476G154S038P510G227'),\n",
       "       ('ID00081', 'R684T571V567C165E284'),\n",
       "       ('ID00082', 'O892E928X200U764B882'),\n",
       "       ('ID00083', 'M835X797I217G943Z946'),\n",
       "       ('ID00084', 'I076F403L593T354Z899'),\n",
       "       ('ID00085', 'C142M331H955C224N610'),\n",
       "       ('ID00086', 'H615W497H403N045Q954'),\n",
       "       ('ID00087', 'Y505K131U951F526D527'),\n",
       "       ('ID00088', 'X631U665B143V813I644'),\n",
       "       ('ID00089', 'W098I207J208Y724E346'),\n",
       "       ('ID00090', 'M869K290J937X491H180'),\n",
       "       ('ID00091', 'M309M856M870B158M892'),\n",
       "       ('ID00092', 'Z170M149B636Q638I446'),\n",
       "       ('ID00093', 'C663U461I006M315X109'),\n",
       "       ('ID00094', 'D279J027C669L420F594'),\n",
       "       ('ID00095', 'F340D780O362J975Z500'),\n",
       "       ('ID00096', 'K579V471C969E098L470'),\n",
       "       ('ID00097', 'Z168C589V971H425Q825'),\n",
       "       ('ID00098', 'K772K161W276H361E555'),\n",
       "       ('ID00099', 'W869O444X940P559K511')], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b9eac-9dc1-441b-812f-fbac3658477f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
