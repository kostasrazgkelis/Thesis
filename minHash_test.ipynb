{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51f18bf1-b27a-46ae-87e3-2e1671103fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash\n",
    "from packages.generateDataSets import SyntheticMatcherDataset\n",
    "from packages.calculateStatistics import DatasetEvaluator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae19e46-e977-4aab-a84c-4cb3147cdb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar records to id1: ['id2', 'id1']\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# Sample records\n",
    "records = {\n",
    "    \"id1\": [\"N780\", \"C459\", \"H866\", \"G186\", \"E487\"],\n",
    "    \"id2\": [\"N780\", \"C459\", \"H866\", \"V186\", \"S537\"],\n",
    "    \"id3\": [\"L123\", \"M456\", \"X789\"]\n",
    "}\n",
    "\n",
    "# Create LSH index\n",
    "lsh = MinHashLSH(threshold=0.4, num_perm=128)\n",
    "minhashes = {}\n",
    "\n",
    "# Build index\n",
    "for record_id, tokens in records.items():\n",
    "    m = MinHash(num_perm=128)\n",
    "    for token in tokens:\n",
    "        m.update(token.encode('utf8'))\n",
    "    minhashes[record_id] = m\n",
    "    lsh.insert(record_id, m)\n",
    "\n",
    "# Query: find similar records to id1\n",
    "query_result = lsh.query(minhashes[\"id1\"])\n",
    "print(f\"Similar records to id1: {query_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c651350d-7738-420b-bf95-0f006ec5e594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ID00005\": [\n",
      "        \"ID00005\"\n",
      "    ],\n",
      "    \"ID00009\": [\n",
      "        \"ID00009\"\n",
      "    ],\n",
      "    \"ID00007\": [\n",
      "        \"ID00007\"\n",
      "    ],\n",
      "    \"ID00004\": [\n",
      "        \"ID00004\"\n",
      "    ],\n",
      "    \"ID10004\": [],\n",
      "    \"NEW72378\": [\n",
      "        \"NEW80187\"\n",
      "    ],\n",
      "    \"ID00008\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "import pandas as pd\n",
    "\n",
    "# Your input data\n",
    "data1 = [\n",
    "    ['ID00005', 'N039', 'E298', 'Q412', 'V409', 'R232'],\n",
    "    ['ID00009', 'R822', 'W179', 'H017', 'P323', 'F298'],\n",
    "    ['ID00007', 'R449', 'X716', 'M948', 'G667', 'S702'],\n",
    "    ['ID00004', 'N002', 'E396', 'N843', 'I458', 'S719'],\n",
    "    ['ID10004', 'N002', 'E396', 'N853', 'I623', 'S569'],\n",
    "    ['NEW72378', 'J547', 'B222', 'G492', 'R551', 'S490'],\n",
    "    ['ID00008', 'N322', 'K685', 'T442', 'C825', 'W967'],\n",
    "]\n",
    "\n",
    "data2 = [\n",
    "    ['ID00005', 'R746', 'E298', 'Q412', 'L291', 'R232'],\n",
    "    ['ID00009', 'R822', 'W179', 'H017', 'P323', 'F298'],\n",
    "    ['ID00007', 'Z011', 'X716', 'M948', 'W967', 'S702'],\n",
    "    ['ID00004', 'N002', 'E396', 'N843', 'V935', 'S719'],\n",
    "    ['ID10004', 'N002', 'E396', 'N553', 'I453', 'S459'],\n",
    "    ['NEW80187', 'J547', 'B222', 'G492', 'W673', 'S490'],\n",
    "    ['NEW30110', 'N322', 'K685', 'T432', 'C225', 'W967'],\n",
    "]\n",
    "\n",
    "# Create MinHash function\n",
    "def get_minhash(features, num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for token in features:\n",
    "        m.update(token.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "# Build LSH index with data2\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "minhash_dict = {}\n",
    "\n",
    "for row in data2:\n",
    "    idx, features = row[0], row[1:]\n",
    "    m = get_minhash(features)\n",
    "    lsh.insert(idx, m)\n",
    "    minhash_dict[idx] = m\n",
    "\n",
    "# Query each data1 record\n",
    "matches = {}\n",
    "for row in data1:\n",
    "    idx, features = row[0], row[1:]\n",
    "    m = get_minhash(features)\n",
    "    result = lsh.query(m)\n",
    "    matches[idx] = result\n",
    "\n",
    "# Print results\n",
    "import json\n",
    "print(json.dumps(matches, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30b3b63c-f810-4846-b41c-48932b560563",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expecting minhash with length 256, got 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     idx, features \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m0\u001b[39m], row[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     20\u001b[0m     m \u001b[38;5;241m=\u001b[39m get_minhash(features)\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mlsh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     minhash_dict[idx] \u001b[38;5;241m=\u001b[39m m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Query each data1 record\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/datasketch/lsh.py:227\u001b[0m, in \u001b[0;36mMinHashLSH.insert\u001b[0;34m(self, key, minhash, check_duplication)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert\u001b[39m(\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    210\u001b[0m     key: Hashable,\n\u001b[1;32m    211\u001b[0m     minhash: Union[MinHash, WeightedMinHash],\n\u001b[1;32m    212\u001b[0m     check_duplication: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    213\u001b[0m ):\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    Insert a key to the index, together with a MinHash or Weighted MinHash\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    of the set referenced by the key.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminhash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_duplication\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_duplication\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/datasketch/lsh.py:296\u001b[0m, in \u001b[0;36mMinHashLSH._insert\u001b[0;34m(self, key, minhash, check_duplication, buffer)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_insert\u001b[39m(\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    290\u001b[0m     key: Hashable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     buffer: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    294\u001b[0m ):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(minhash) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting minhash with length \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mlen\u001b[39m(minhash))\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepickle:\n\u001b[1;32m    300\u001b[0m         key \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(key)\n",
      "\u001b[0;31mValueError\u001b[0m: Expecting minhash with length 256, got 128"
     ]
    }
   ],
   "source": [
    "dataset = SyntheticMatcherDataset(size=10000 , true_positive_ratio=0.70, threshold=3)\n",
    "df1, df2 = dataset.df1.values.tolist(), dataset.df2.values.tolist()\n",
    "expected = dataset.expected\n",
    "\n",
    "# Create MinHash function\n",
    "def get_minhash(features, num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for token in features:\n",
    "        m.update(token.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "# Build LSH index with data2\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=256)\n",
    "minhash_dict = {}\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "for row in df2:\n",
    "    idx, features = row[0], row[1:]\n",
    "    m = get_minhash(features)\n",
    "    lsh.insert(idx, m)\n",
    "    minhash_dict[idx] = m\n",
    "\n",
    "# Query each data1 record\n",
    "matches = {}\n",
    "for row in df1:\n",
    "    idx, features = row[0], row[1:]\n",
    "    m = get_minhash(features)\n",
    "    result = lsh.query(m)\n",
    "    matches[idx] = result\n",
    "\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "979a4c46-84b3-4b25-af04-b821ea9eeae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ID00005\": [\n",
      "        \"ID00005\"\n",
      "    ],\n",
      "    \"ID00009\": [\n",
      "        \"ID00009\"\n",
      "    ],\n",
      "    \"ID00007\": [\n",
      "        \"ID00007\"\n",
      "    ],\n",
      "    \"ID00004\": [\n",
      "        \"ID00004\"\n",
      "    ],\n",
      "    \"ID10004\": [],\n",
      "    \"NEW72378\": [\n",
      "        \"NEW80187\"\n",
      "    ],\n",
      "    \"ID00008\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(matches, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bd673-3ce2-473b-aa99-a57a01a917cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
