{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa1b45-7b04-4cc2-a017-5cfbf9ffec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from packages.utils.generate_data_set import SyntheticMatcherDataset\n",
    "from packages.pandas.pandas_pipeline import DatasetEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c68bc-bef6-402f-8add-6fbeddfb9a87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess took 0.0022 seconds\n",
      "evaluate took 0.0008 seconds\n",
      "calculateStatistics took 0.0000 seconds\n",
      "Expected: {'gt': 5, 'tp': 4, 'fp': 2, 'fn': 1}\n",
      "Ground Truth Size: 5\n",
      "True Positives: 4\n",
      "False Positives: 2\n",
      "False Negatives: 1\n",
      "Precision: 0.6667\n",
      "Recall: 0.8000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data for df1\n",
    "data1 = [\n",
    "    ['ID00005', 'N039', 'E298', 'Q412', 'V409', 'R232'], #TP1\n",
    "    ['ID00009', 'R822', 'W179', 'H017', 'P323', 'F298'], #TP2\n",
    "    ['ID00007', 'R449', 'X716', 'M948', 'G667', 'S702'], #TP3\n",
    "    ['ID00004', 'N002', 'E396', 'N843', 'I458', 'S719'], #TP4\n",
    "    ['ID10004', 'N002', 'E396', 'N853', 'I623', 'S569'], #FN1\n",
    "    ['NEW72378', 'J547', 'B222', 'G492', 'R551', 'S490'], #FP1\n",
    "    ['ID00008', 'N322', 'K685', 'T442', 'C825', 'W967'], #FP2\n",
    "    ['ID00000', 'W815', 'L281', 'R155', 'F768', 'B914'],\n",
    "    ['ID00001', 'C172', 'B326', 'X400', 'M508', 'O776'],\n",
    "    ['ID00002', 'V683', 'C265', 'J127', 'D589', 'F482'],\n",
    "    ['ID00003', 'E851', 'P721', 'F745', 'D863', 'K229'],\n",
    "    ['ID00016', 'T873', 'D670', 'U046', 'Z181', 'X621'],\n",
    "    ['ID00017', 'F327', 'G856', 'E567', 'O929', 'Q721'],\n",
    "    ['ID00010', 'O283', 'T723', 'Z034', 'V319', 'X338'],\n",
    "]\n",
    "\n",
    "# Data for df2\n",
    "data2 = [\n",
    "    ['ID00005', 'R746', 'E298', 'Q412', 'L291', 'R232'], #TP1\n",
    "    ['ID00009', 'R822', 'W179', 'H017', 'P323', 'F298'], #TP2\n",
    "    ['ID00007', 'Z011', 'X716', 'M948', 'W967', 'S702'], #TP3\n",
    "    ['ID00004', 'N002', 'E396', 'N843', 'V935', 'S719'], #TP4\n",
    "    ['ID10004', 'N002', 'E396', 'N553', 'I453', 'S459'], #FN1\n",
    "    ['NEW80187', 'J547', 'B222', 'G492', 'W673', 'S490'], #FP1\n",
    "    ['NEW30110', 'N322', 'K685', 'T432', 'C225', 'W967'], #FP2\n",
    "    ['NEW72832', 'F875', 'Q768', 'H822', 'Z154', 'X678'], \n",
    "    ['NEW30110', 'R560', 'C434', 'M687', 'Q689', 'Q863'],\n",
    "    ['NEW81243', 'R762', 'N687', 'A109', 'K476', 'R637'],\n",
    "    ['NEW52689', 'A089', 'V733', 'W158', 'A640', 'H331'],\n",
    "    ['NEW67368', 'Z079', 'J617', 'G878', 'W111', 'Q500'],\n",
    "    ['NEW72348', 'J547', 'B222', 'G492', 'R551', 'S490'],\n",
    "    ['NEW34469', 'Y990', 'H898', 'W673', 'L967', 'M829'],\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "columns = [0, 1, 2, 3, 4, 5]\n",
    "df1 = pd.DataFrame(data1, columns=columns)\n",
    "df2 = pd.DataFrame(data2, columns=columns)\n",
    "expected = {'gt': 5, 'tp': 4, 'fp': 2, 'fn': 1}\n",
    "\n",
    "evaluator = DatasetEvaluator(df1, df2, expected=expected, trim=0, threshold=3)\n",
    "evaluator.preprocess()\n",
    "evaluator.evaluate()\n",
    "evaluator.calculate_statistics()\n",
    "evaluator.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16be8e8c-f259-4f3f-a3f3-a66b8be074af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1 avg time: 578.75 sec\n",
      "Pipeline 2 avg time: 114.03 sec\n",
      "-80.30% reduce time ---> improvement\n"
     ]
    }
   ],
   "source": [
    "time1 =  578.75\n",
    "time2 = 114.03\n",
    "\n",
    "print(f\"Pipeline 1 avg time: {time1} sec\")\n",
    "print(f\"Pipeline 2 avg time: { time2} sec\")\n",
    "\n",
    "if time1 < time2:\n",
    "    print(f\"{(1 - time1/time2)*100:.2f}% increase time ---> not improvement\")\n",
    "else:\n",
    "    print(f\"-{(1 - time2/time1)*100:.2f}% reduce time ---> improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d584d-8e52-4c56-ae07-eb0be03cd46b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "1250 Elapsed Time: 1.72 seconds\n",
    "2500 Elapsed Time: 7.02 seconds \n",
    "5000 Elapsed Time: 28.09 seconds\n",
    "10000 Elapsed Time: 114.03 seconds\n",
    "\n",
    "\n",
    "1250 Elapsed Time: 6.29 seconds\n",
    "2500 Elapsed Time: 26 seconds \n",
    "5000 Elapsed Time: 108.56 seconds\n",
    "10000 Elapsed Time: 578.75 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d83a7-fcda-424e-92c1-f666c1be7136",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess took 0.1427 seconds\n",
      "evaluate took 0.7788 seconds\n",
      "calculateStatistics took 0.0002 seconds\n",
      "Expected: {'gt': 125, 'tp': 93, 'fp': 69, 'fn': 32}\n",
      "Ground Truth Size: 125\n",
      "True Positives: 93\n",
      "False Positives: 69\n",
      "False Negatives: 32\n",
      "Precision: 0.5741\n",
      "Recall: 0.7440\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset = SyntheticMatcherDataset(size=500 ,  ground_truth_ratio=0.25, datasets_ratio = (1, 2), true_positive_ratio=0.75, threshold=3)\n",
    "df1, df2 = dataset.df1, dataset.df2\n",
    "expected = dataset.expected\n",
    "\n",
    "evaluator = DatasetEvaluator(df1, df2, expected, threshold=3, trim=0, match_column=\"id\")\n",
    "evaluator.preprocess()\n",
    "evaluator.evaluate()\n",
    "evaluator.calculate_statistics()\n",
    "evaluator.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "feeba8ad-8cee-4741-8298-705d4ca5a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best column triplet (least unique values): ('col1', 'col2', 'col3')\n",
      "Number of unique combinations: 10000\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "columns = ['col1', 'col2', 'col3', 'col4', 'col5']\n",
    "\n",
    "# Store result as (column_triplet, unique_count)\n",
    "unique_counts = []\n",
    "\n",
    "for cols in combinations(columns, 3):\n",
    "    count = df2[list(cols)].agg(''.join, axis=1).nunique()\n",
    "    unique_counts.append((cols, count))\n",
    "\n",
    "# Find the combination with the minimum unique count\n",
    "best_combination = min(unique_counts, key=lambda x: x[1])\n",
    "\n",
    "print(\"Best column triplet (least unique values):\", best_combination[0])\n",
    "print(\"Number of unique combinations:\", best_combination[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
