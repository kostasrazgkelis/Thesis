{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54eabc84-d793-4183-920d-ac4f20ab2902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jellyfish in /opt/conda/lib/python3.11/site-packages (1.2.0)\n",
      "Requirement already satisfied: faker in /opt/conda/lib/python3.11/site-packages (37.1.0)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.11/site-packages (from faker) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish\n",
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fa1b45-7b04-4cc2-a017-5cfbf9ffec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import jellyfish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "import math\n",
    "from itertools import combinations, product\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import defaultdict\n",
    "from packages.generateDataSets import SyntheticMatcherDataset\n",
    "from packages.calculateStatistics import DatasetEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a756b54-8c72-4213-9683-0f2a3ea10c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263d1dfd-42da-463a-9eed-86cf9556e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, df1: pd.DataFrame, \n",
    "                 df2: pd.DataFrame, \n",
    "                 matchColumn: str, \n",
    "                 on: List = [],\n",
    "                 method: str = 'column', \n",
    "                 threshold: float  = 0.6):\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "        self.on = on\n",
    "        self.threshold = threshold\n",
    "\n",
    "        if method not in [\"concat\", \"column\"]:\n",
    "            raise ValueError(f\"Method '{method}' is not correct.\")\n",
    "        self.method = method\n",
    "\n",
    "    \n",
    "        if matchColumn not in self.df1.columns or matchColumn not in self.df2.columns:\n",
    "            raise ValueError(f\"Column '{matchColumn}' is not found in both DataFrames.\")\n",
    "        self.matchColumn = matchColumn\n",
    "        \n",
    "        self.groundTruth = None\n",
    "        self.totalMatches = None        \n",
    "    \n",
    "    def setGroundTruth(self):\n",
    "        \"\"\"Sets the ground truth based on matching 'id' columns.\"\"\"\n",
    "        self.groundTruth = np.intersect1d(self.df1[self.matchColumn], self.df2[self.matchColumn])\n",
    "\n",
    "    def soundexDfs(self):\n",
    "        \"\"\"Apply soundex transformation to non-id columns.\"\"\"\n",
    "        for df in [self.df1, self.df2]:\n",
    "            for col_name in df.columns:\n",
    "                if col_name != self.matchColumn:\n",
    "                    df[col_name] = df[col_name].apply(lambda x: jellyfish.soundex(str(x)))\n",
    "\n",
    "            if self.method == 'concat':\n",
    "                non_match_columns = [col for col in df.columns if col != self.matchColumn]\n",
    "                df['concatenated'] = df[non_match_columns].apply(lambda row: ''.join(row.astype(str)), axis=1)\n",
    "                df.drop(columns=non_match_columns, inplace=True)\n",
    "\n",
    "    def setTotalMatches(self):\n",
    "        \"\"\"Sets the total matches based on merged DataFrames.\"\"\"\n",
    "        \n",
    "        # if self.method == 'concat':\n",
    "        #     self.totalMatches = self.df1.merge(self.df2, how=\"inner\", on=['concatenated']).to_numpy()\n",
    "        # else:   \n",
    "        #     self.totalMatches = self.df1.merge(self.df2, how=\"outer\", on=self.on + [self.matchColumn]).to_numpy()\n",
    "\n",
    "        self.totalMatches =  self.df1.merge(pd.concat([self.df1, self.df2]), how='outer', on=self.on)[[\"0_y\"] + self.on]\n",
    "        self.totalMatches.rename(columns={'0_y': self.matchColumn}, inplace=True)\n",
    "        \n",
    "    def printStatistics(self):\n",
    "        \"\"\"Print statistics (True Positives, False Positives, Precision).\"\"\"\n",
    "        myStatistics = self.Statistics(groundTruth=self.groundTruth, \n",
    "                                       totalMatches=self.totalMatches, \n",
    "                                       threshold=self.threshold, \n",
    "                                       on=self.on, \n",
    "                                       matchColumn=self.matchColumn)\n",
    "        myStatistics.calculate()\n",
    "\n",
    "    # Inner class Statistics\n",
    "    class Statistics:\n",
    "        def __init__(self,\n",
    "                     groundTruth: pd.DataFrame, \n",
    "                     totalMatches: pd.DataFrame, \n",
    "                     threshold : float = 0.8,\n",
    "                     matchColumn: str | int = 1,\n",
    "                     on: List =[]):\n",
    "            self.groundTruth = pd.DataFrame(groundTruth)\n",
    "            self.totalMatches = pd.DataFrame(totalMatches)\n",
    "            self.threshold = threshold\n",
    "            self.matchColumn = matchColumn\n",
    "            self.on = on\n",
    "\n",
    "            self._setThresholdValues()\n",
    "            \n",
    "        def calculate(self):\n",
    "            # self.result = self.totalMatches.groupby(self.matchColumn)\\\n",
    "            #         .filter(lambda x : len(x) >=2)\\\n",
    "            #         .groupby(self.matchColumn)\\\n",
    "            #         .apply(lambda x: x.iloc[:, 1:].apply(lambda x: x.nunique() == 1)).sum(axis=1)\n",
    "\n",
    "            duplicates = self.totalMatches[self.totalMatches[[1,2,3,4,5]].duplicated(keep=False)].sort_values(by=[1,2,3,4,5])\n",
    "        \n",
    "            # Function to check if two rows match at least 3/5 columns\n",
    "            def is_duplicate(row1, row2):\n",
    "                return sum(row1 == row2) >=  self.matchingRows  # At least 3 matches out of 5\n",
    "\n",
    "            # print(duplicates)\n",
    "            \n",
    "            duplicate_pairs = []\n",
    "            # Find duplicates\n",
    "            for i in range(len(duplicates)):\n",
    "                for j in range(i + 1, len(duplicates)):  # Compare only unique pairs\n",
    "                    if is_duplicate(duplicates.iloc[i, 1:], duplicates.iloc[j, 1:]):\n",
    "                        duplicate_pairs.append((i, j, duplicates.iloc[i, 0] == duplicates.iloc[j, 0]))  # Store (index1, index2, same_id)\n",
    "\n",
    "            # Count same ID and different ID duplicates\n",
    "            tp = sum(1 for _, _, same_id in duplicate_pairs if same_id)\n",
    "            fp = len(duplicate_pairs) - same_id_count\n",
    "            fn = self.groundTruth.size - tp\n",
    "            \n",
    "            precision = tp / (tp + fp) if tp + fp != 0 else 0 \n",
    "            recall = tp / (tp + fn)  if tp + fn != 0 else 0\n",
    "            f1_score = (2 * precision * recall) / (precision + recall)\n",
    "            \n",
    "            print(\"Total Possible Mathces:\", self.groundTruth.size)\n",
    "            print(\"True Positives (TP):\", tp)\n",
    "            print(\"False Positives (FP):\", fp)\n",
    "            print(\"False Negatives (FN):\", fn)\n",
    "            print(\"Precision:\", f\"{precision:.4f}\")\n",
    "            print(\"Recall:\", f\"{recall:.4f}\")\n",
    "            print(\"F1-score:\", f\"{f1_score:.4f}\")\n",
    "\n",
    "        def _matchingAlgorithm(self, group):\n",
    "            return group.nunique() == 1\n",
    "            \n",
    "        def _setThresholdValues(self) -> List:\n",
    "            size = len(self.totalMatches.columns) - 1\n",
    "            limit = math.floor(self.threshold * size)\n",
    "            \n",
    "            print(f\"We accept at least {limit}/{size} as matches!\") \n",
    "            self.matchingRows = limit\n",
    "            # return [i for i in range(size, limit , -1)]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d804a1c-5175-4398-b9a5-30b12ac8386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH  =  \"data/\"\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(PATH, 'df1.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "df2 = pd.read_csv(os.path.join(PATH, 'df5.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "\n",
    "# Run pipeline and see statistics\n",
    "pipeline = MyClass(df1, df2, matchColumn=0, on=[1,2,3,4,5], method=\"column\", threshold = 0.6) #  --> this means at least 3/5 of the fields must match \n",
    "pipeline.setGroundTruth()\n",
    "pipeline.soundexDfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12492b90-54e7-4707-9c6d-cf1070265fdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m DatasetEvaluator(pipeline\u001b[38;5;241m.\u001b[39mdf1, pipeline\u001b[38;5;241m.\u001b[39mdf2, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m evaluator\u001b[38;5;241m.\u001b[39mprintResults()\n",
      "File \u001b[0;32m~/work/packages/calculateStatistics.py:50\u001b[0m, in \u001b[0;36mDatasetEvaluator.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m df2_buckets:\n\u001b[1;32m     49\u001b[0m     row2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(data[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow2\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     51\u001b[0m         df2_buckets[data]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/work/packages/calculateStatistics.py:26\u001b[0m, in \u001b[0;36mDatasetEvaluator._is_similar\u001b[0;34m(self, row1, row2)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_similar\u001b[39m(\u001b[38;5;28mself\u001b[39m, row1, row2):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39marray(row1) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray(row2)) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluator = DatasetEvaluator(pipeline.df1, pipeline.df2, threshold=4)\n",
    "evaluator.evaluate()\n",
    "evaluator.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3013db6-b66b-491b-bfe2-d07daf5e4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 2\n",
    "Expected: {}\n",
    "Ground Truth Size: 25000\n",
    "True Positives: 24977\n",
    "False Positives: 74055\n",
    "False Negatives: 23\n",
    "Precision: 0.2522\n",
    "Recall: 0.9991\n",
    "Elapsed Time: 4861.54 seconds\n",
    "\n",
    "thresh = 3\n",
    "Expected: {}\n",
    "Ground Truth Size: 25000\n",
    "True Positives: 19125\n",
    "False Positives: 16323\n",
    "False Negatives: 5875\n",
    "Precision: 0.5395\n",
    "Recall: 0.7650\n",
    "Elapsed Time: 80256.47 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a3d4cf-36e3-4b0e-96eb-5f4923a1274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import jellyfish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "import math\n",
    "from itertools import combinations, product\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import defaultdict\n",
    "from packages.generateDataSets import SyntheticMatcherDataset\n",
    "from packages.calculateStatistics import DatasetEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "603541e9-4f75-435c-86aa-7e45489ef7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create two datasets with slight variations\n",
    "# Data have 3 matches and one \n",
    "data1 = {\n",
    "    0: [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    1: [\"Kostas\", \"Maria\", \"John\", \"Sophia\", \"George\", \"Eleni\", \"Michael\", \"Anna\", \"Chris\", \"Dimitris\"],\n",
    "    2: [\"Razgkelis\", \"Papadopoulos\", \"Smith\", \"Johnson\", \"Pavlou\", \"Nikolaou\", \"Brown\", \"Miller\", \"Taylor\", \"Andreas\"],\n",
    "    3: [\"Orestiada\", \"Thessaloniki\", \"Grevena\", \"Athina\", \"Aleksandroupoli\", \"Giannena\", \"Larissa\", \"Komotini\", \"Trikala\", \"Kozani\"],\n",
    "    4: ['Jennifer Lights', 'Brandon Lakes', 'Aguilar Stravenue', 'Richardson Ferry', 'Freeman Way', \n",
    "        'Gabrielle Underpass', 'Burns Summit', 'Heather Village', 'Jamie Common', 'Greg Lock'],\n",
    "    5:  ['Cooper and Sons', 'Pope LLC', 'Fowler-Smith', 'Torres PLC', 'Jones LLC', 'White, Duncan and Robinson', 'Hayden Inc', \n",
    "         'Wilson and Sons', 'Peterson, Smith and Robinson','Hudson, Phelps and Day'],\n",
    "    \n",
    "}\n",
    "\n",
    "data2 = {\n",
    "    0: [101, 202, 203, 204, 205, 206, 207, 208, 209, 110],\n",
    "    1: [\"Kistas\", \"Maria\", \"John\", \"Sophasdia\", \"Giorge\", \"Elendsi\", \"Micheal\", \"Ana\", \"Khris\", \"Dimtris\"],\n",
    "    2: [\"Rozgkliiis\", \"Papadopoulos\", \"Smith\", \"Johnson\", \"Pavlodvu\", \"Nikolaou\", \"Batrrroun\", \"antMiler\", \"Tttayloor\", \"Andres\"],\n",
    "    3: [\"Orestiada\", \"Thessaloniki\", \"Grevena\", \"Athina\", \"Aleksandrouasdpoli\", \"Gianasdna\", \"Larasdissa\", \"Komasdini\", \"Trsadala\", \"Koxani\"],\n",
    "    4: [\"Jnnfer Lights\", 'Brandon Lakes', 'Aguilar Stravenue', 'RichardasasdaFerry', 'Freasdeman Way', 'Gabrielle Underpass', 'Burasdas mmit', 'Heatasasdllage', 'JamasdCommon', 'Grg Lck'],\n",
    "    5:  ['Cpeeer and ons', 'Pope LLC', 'Fowler-Smith', 'Torvasd PLC', 'Jonasda LLC', 'Whitasddvuncan and Robinson', 'Hayasdasv Inc', \n",
    "         'Wasdvand Sons', 'Petersosdvaith and Robinson','Htsn, Phelps and Day'],\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d6b22753-5166-46e7-b4d2-b0875bc3e1e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrame\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Run pipeline and see statistics\n",
    "pipeline = MyClass(df1, df2, matchColumn=0, on=[1, 2, 3, 4, 5], threshold=0.4)\n",
    "pipeline.setGroundTruth()\n",
    "pipeline.soundexDfs()\n",
    "pipeline.setTotalMatches()\n",
    "# pipeline.printStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b590ab60-2d2c-431e-b8a2-81132567745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    101\n",
      "Name: 0, dtype: int64\n",
      "0    110\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b7d2014b-6f08-4937-8171-ca33dd0e6936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH  =  \"data/\"\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(PATH, 'df1.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "df2 = pd.read_csv(os.path.join(PATH, 'df5.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "\n",
    "# Run pipeline and see statistics\n",
    "pipeline = MyClass(df1, df2, matchColumn=0, on=[1,2,3,4,5], method=\"column\", threshold = 0.6) #  --> this means at least 3/5 of the fields must match \n",
    "pipeline.setGroundTruth()\n",
    "pipeline.soundexDfs()\n",
    "# pipeline.setTotalMatches()\n",
    "# pipeline.printStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "434913f3-5136-4702-83b2-efcd0c1ea41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 133\n",
      "False Positives: 5\n",
      "False Negatives: 67\n",
      "Precision: 0.9638\n",
      "Recall: 0.6650\n",
      "Elapsed Time: 52.30 seconds\n"
     ]
    }
   ],
   "source": [
    "def is_similar(row1, row2, threshold=4):\n",
    "    return np.sum(row1[1:] == row2[1:]) >= threshold  # Compare columns 1-5\n",
    "\n",
    "\n",
    "# Function to process chunks\n",
    "def process_chunk(chunk_row_pairs, df1, df2, ground_truth):\n",
    "    totalMatches = []\n",
    "    for id1, id2 in chunk_row_pairs:\n",
    "        if is_similar(df1[df1[:, 0] == id1][0], df2[df2[:, 0] == id2][0], 3):\n",
    "            totalMatches.append((id1, id2))\n",
    "\n",
    "    # Calculate tp, fp, fn for this chunk\n",
    "    tp = sum(1 for x, y in totalMatches if x == y and x in ground_truth)\n",
    "    fp = len(totalMatches) - tp\n",
    "    fn = len(ground_truth) - tp\n",
    "    \n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "size = 200\n",
    "\n",
    "df1 = pd.concat([pipeline.df1.iloc[:size], pipeline.df1.iloc[25_000:25000 + size * 3]]).to_numpy()\n",
    "df2 = pd.concat([pipeline.df2.iloc[:size], pipeline.df2.iloc[25_000:25000 + size * 3]]).to_numpy()\n",
    "\n",
    "ground_truth = np.intersect1d(df1[:, 0], df2[:, 0])\n",
    "\n",
    "# Split row pairs into chunks\n",
    "chunk_size = 1000  # Adjust chunk size based on memory and performance\n",
    "\n",
    "# Process in chunks\n",
    "total_tp, total_fp, total_fn = 0, 0, 0\n",
    "for i in range(0, len(df1), chunk_size):\n",
    "    # Generate row pairs for the current chunk\n",
    "    chunk_row_pairs = list(product(df1[i:i + chunk_size, 0], df2[:, 0]))\n",
    "    tp, fp, fn = process_chunk(chunk_row_pairs, df1, df2, ground_truth)\n",
    "    \n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"True Positives: {total_tp}\")\n",
    "print(f\"False Positives: {total_fp}\")\n",
    "print(f\"False Negatives: {total_fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ff153e9-8134-4db8-bddf-edac31f7f3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pipeline.df1.iloc[:250], pipeline.df1.iloc[25_000:25000 + 750]]).to_numpy().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14c5636f-3a0d-46cb-97c1-3691cfe54de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1745433922.1039279"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([7.85, 8.35, 9.89, 8.78, 8.87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "370fd24f-2174-4c55-b124-ad7db2765a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 330\n",
      "False Positives: 50\n",
      "False Negatives: 170\n",
      "Precision: 0.8684\n",
      "Recall: 0.6600\n",
      "Elapsed Time: 26.34 seconds\n"
     ]
    }
   ],
   "source": [
    "def is_similar(row1, row2, threshold=3):\n",
    "    return np.sum(row1[1:] == row2[1:]) >= threshold\n",
    "\n",
    "def process_chunk(chunk_row_pairs, df1_dict, df2_dict, ground_truth, threshold=3):\n",
    "    total_matches = []\n",
    "    for id1, id2 in chunk_row_pairs:\n",
    "        row1 = df1_dict.get(id1)\n",
    "        row2 = df2_dict.get(id2)\n",
    "        if row1 is not None and row2 is not None:\n",
    "            if is_similar(row1, row2, threshold):\n",
    "                total_matches.append((id1, id2))\n",
    "\n",
    "    tp = sum(1 for id1, id2 in total_matches if id1 == id2 and id1 in ground_truth)\n",
    "    fp = len(total_matches) - tp\n",
    "    \n",
    "    return tp, fp\n",
    "    \n",
    "# Preprocess\n",
    "size = 500\n",
    "df1 = pd.concat([pipeline.df1.iloc[:size], pipeline.df1.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "df2 = pd.concat([pipeline.df2.iloc[:size], pipeline.df2.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "\n",
    "df1_dict = {row[0]: row for row in df1}\n",
    "df2_dict = {row[0]: row for row in df2}\n",
    "ground_truth = set(np.intersect1d(df1[:, 0], df2[:, 0]))\n",
    "\n",
    "chunk_size = 500\n",
    "total_tp = total_fp = total_fn = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(df1), chunk_size):\n",
    "    chunk_ids = df1[i:i + chunk_size, 0]\n",
    "    chunk_row_pairs = list(product(chunk_ids, df2[:, 0]))\n",
    "     tp, fp = process_chunk(chunk_row_pairs, df1_dict, df2_dict, ground_truth)\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "\n",
    "total_fn = len(ground_truth) - total_tp\n",
    "\n",
    "precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "\n",
    "print(f\"True Positives: {total_tp}\")\n",
    "print(f\"False Positives: {total_fp}\")\n",
    "print(f\"False Negatives: {total_fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8b05e-b4b7-4e3e-8b95-a8195a080432",
   "metadata": {},
   "outputs": [],
   "source": [
    "148.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e2e5719-6aa9-4c05-b735-648c0af65c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7da9b7-4e7b-4d18-931e-121cc76efd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "\n",
    "True Positives: 3437\n",
    "False Positives: 1697\n",
    "False Negatives: 1563\n",
    "Precision: 0.6695\n",
    "Recall: 0.6874\n",
    "Elapsed Time: 2599.92 seconds\n",
    "\n",
    "n = 15000\n",
    "\n",
    "True Positives: 9903\n",
    "False Positives: 8710\n",
    "False Negatives: 5097\n",
    "Precision: 0.5320\n",
    "Recall: 0.6602\n",
    "Elapsed Time: 26709.04 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "36185f65-6e33-40ad-a33f-d6a3f658570c",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[333], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# df1 = pd.concat([pipeline.df1.iloc[:size], pipeline.df1.iloc[25000:25000 + size * 3]]).to_numpy()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# df2 = pd.concat([pipeline.df2.iloc[:size], pipeline.df2.iloc[25000:25000 + size * 3]]).to_numpy()\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m ground_truth_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(np\u001b[38;5;241m.\u001b[39mintersect1d(\u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, df2[:,\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     19\u001b[0m df2_buckets \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df2:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3660\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3660\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:5737\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5735\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5736\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5737\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "# Index df2 by a hash on a few features to reduce comparisons\n",
    "# For example, use a subset of columns to pre-group similar rows\n",
    "# Here, we use the first column (ID) as a rough bucket\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def is_similar(row1, row2, threshold=2):\n",
    "    return np.sum(row1[1:] == row2[1:]) >= threshold\n",
    "\n",
    "# Preprocess\n",
    "size = 500\n",
    "df1 = pd.concat([pipeline.df1.iloc[:size], pipeline.df1.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "df2 = pd.concat([pipeline.df2.iloc[:size], pipeline.df2.iloc[25000:25000 + size * 3]]).to_numpy()\n",
    "\n",
    "ground_truth_ids = set(np.intersect1d(df1[:,0], df2[:,0]))\n",
    "\n",
    "df2_buckets = defaultdict(list)\n",
    "for row in df2:\n",
    "    df2_buckets[row[0]].append(row)  # You can change the key to a composite or substring\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "matched = set()\n",
    "\n",
    "for row1 in df1:\n",
    "    candidates = df2_buckets.get(row1[0], [])\n",
    "    for row2 in candidates:\n",
    "        if is_similar(row1, row2, threshold=3):\n",
    "            matched.add((row1[0], row2[0]))  # Track ID pairs\n",
    "            break\n",
    "\n",
    "total_tp = sum(1 for id1, id2 in matched if id1 == id2 and id1 in ground_truth_ids)\n",
    "total_fp = sum(1 for id1, id2 in matched if id1 != id2 or id1 not in ground_truth_ids)\n",
    "total_fn = len(ground_truth_ids) - total_tp \n",
    "\n",
    "precision = total_tp / (total_tp + total_fp) if total_tp + total_fp > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) if total_tp + total_fn > 0 else 0\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"True Positives: {total_tp}\")\n",
    "print(f\"False Positives: {total_fp}\")\n",
    "print(f\"False Negatives: {total_fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d22d65-0dea-4312-9fa9-0d363028687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e61a9ab-5dfa-42af-930f-5322c13c0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH  =  \"data/\"\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(PATH, 'df1.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "df2 = pd.read_csv(os.path.join(PATH, 'df5.csv'), header=None)[[0,1,2,3,4,5]]\n",
    "\n",
    "# Run pipeline and see statistics\n",
    "pipeline = MyClass(df1, df2, matchColumn=0, on=[1,2,3,4,5], method=\"column\", threshold = 0.6) #  --> this means at least 3/5 of the fields must match \n",
    "pipeline.setGroundTruth()\n",
    "pipeline.soundexDfs()\n",
    "\n",
    "evaluator = DatasetEvaluator(pipeline.df1, pipeline.df2, expected, threshold=3)\n",
    "evaluator.evaluate()\n",
    "evaluator.printResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37fcd5ed-4335-4743-84b1-f40ee90eb084",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m DatasetEvaluator(pipeline\u001b[38;5;241m.\u001b[39mdf1, pipeline\u001b[38;5;241m.\u001b[39mdf2, \u001b[43mexpected\u001b[49m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m evaluator\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m      3\u001b[0m evaluator\u001b[38;5;241m.\u001b[39mprintResults()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expected' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "417a7d24-9413-4a66-9007-b0000707ef6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.293463888888887"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80256.47/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a4547-5419-4190-b7a9-56f3d8820988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
