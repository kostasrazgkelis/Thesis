{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a675ae66-25b2-4d30-a536-936af8e47db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import pandas as pd\n",
    "from packages.utils.transformations import MyTransformation as mtf\n",
    "from packages.utils.spark_udfs import soundex_udf\n",
    "from pyspark.sql.functions import col, array_contains\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"example2\").getOrCreate()\n",
    "\n",
    "# Read and rename columns as before\n",
    "def read_with_origin(path, origin):\n",
    "    df = spark.read.csv(path, header=None, inferSchema=True).select([\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\"])\n",
    "    for i in range(6):\n",
    "        df = df.withColumnRenamed(f\"_c{i}\", str(i))\n",
    "    df = df.withColumn(\"origin\", lit(origin))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcdbc20-386a-4b5a-989e-0e7868d24f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "transformer = mtf()\n",
    "match_column = 0\n",
    "cols = ['1', '2', '3', '4', '5']\n",
    "\n",
    "df1 =  pd.read_csv(\"data/df1.csv\", usecols=[0,1,2,3,4,5], header=None)[[0,1,2,3,4,5]].sample(frac=0.1, random_state=42)\n",
    "df2 =  pd.read_csv(\"data/df2.csv\", usecols=[0,1,2,3,4,5], header=None)[[0,1,2,3,4,5]].sample(frac=0.1, random_state=42)\n",
    "\n",
    "_expected = {\"gt\": 2531, \"tp\":1775, \"fp\":92, \"fn\":756}\n",
    "\n",
    "transformer.apply_soundex(df1, match_column = 0)\n",
    "transformer.apply_soundex(df2, match_column = 0)\n",
    "\n",
    "df1['origin'] =  1\n",
    "df2['origin'] =  2\n",
    "\n",
    "spark_df1 = spark.createDataFrame(df1)\n",
    "spark_df2 = spark.createDataFrame(df2)\n",
    "\n",
    "concatData = spark_df1.union(spark_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5003caf5-a985-4d14-ace7-542a99db0e56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "# Data for df1\n",
    "data1 = [\n",
    "    [\"ID00005\", \"N039\", \"E298\", \"Q412\", \"V409\", \"R232\"],  # TP1\n",
    "    [\"ID00009\", \"R822\", \"W179\", \"H017\", \"P323\", \"F298\"],  # TP2\n",
    "    [\"ID00007\", \"R449\", \"X716\", \"M948\", \"G667\", \"S702\"],  # TP3\n",
    "    [\"ID00004\", \"N002\", \"E396\", \"N843\", \"I458\", \"S719\"],  # TP4\n",
    "    [\"ID10004\", \"N002\", \"E396\", \"N853\", \"I623\", \"S569\"],  # FN1\n",
    "    [\"ID50004\", \"J547\", \"B222\", \"G492\", \"R551\", \"S490\"],  # FP1\n",
    "    [\"IDTIE00\", \"N322\", \"K685\", \"T443\", \"C225\", \"W947\"],  # FP-tie: this should be skipped\n",
    "    [\"ID50008\", \"N322\", \"K685\", \"T443\", \"C225\", \"W967\"],  # FP2\n",
    "    [\"ID00000\", \"W815\", \"L281\", \"R155\", \"F768\", \"B914\"],\n",
    "    [\"ID00001\", \"C172\", \"B326\", \"X400\", \"M508\", \"O776\"],\n",
    "    [\"ID00002\", \"V683\", \"C265\", \"J127\", \"D589\", \"F482\"],\n",
    "    [\"ID00003\", \"E851\", \"P721\", \"F745\", \"D863\", \"K229\"],\n",
    "    [\"ID00016\", \"T873\", \"D670\", \"U046\", \"Z181\", \"X621\"],\n",
    "    [\"ID00017\", \"F327\", \"G856\", \"E567\", \"O929\", \"Q721\"],\n",
    "    [\"ID00010\", \"O283\", \"T723\", \"Z034\", \"V319\", \"X338\"],\n",
    "]\n",
    "\n",
    "# Data for df2\n",
    "data2 = [\n",
    "    [\"ID00005\", \"R746\", \"E298\", \"Q412\", \"L291\", \"R232\"],  # TP1\n",
    "    [\"ID00009\", \"R822\", \"W179\", \"H017\", \"P323\", \"F298\"],  # TP2\n",
    "    [\"ID00007\", \"Z011\", \"X716\", \"M948\", \"W967\", \"S702\"],  # TP3\n",
    "    [\"ID00004\", \"N002\", \"E396\", \"N843\", \"V935\", \"S719\"],  # TP4\n",
    "    [\"ID10004\", \"N002\", \"E396\", \"N553\", \"I453\", \"S459\"],  # FN1\n",
    "    [\"NEW80187\", \"J547\", \"B222\", \"G492\", \"W673\", \"S490\"],  # FP1\n",
    "    [\"NEW30110\", \"N322\", \"K685\", \"T432\", \"C225\", \"W967\"],  # FP2\n",
    "    [\"NEW72832\", \"F875\", \"Q768\", \"H822\", \"Z154\", \"X678\"],\n",
    "    [\"NEW30110\", \"R560\", \"C434\", \"M687\", \"Q689\", \"Q863\"],\n",
    "    [\"NEW81243\", \"R762\", \"N687\", \"A109\", \"K476\", \"R637\"],\n",
    "    [\"NEW52689\", \"A089\", \"V733\", \"W158\", \"A640\", \"H331\"],\n",
    "    [\"NEW67368\", \"Z079\", \"J617\", \"G878\", \"W111\", \"Q500\"],\n",
    "    [\"NEW72348\", \"J547\", \"B222\", \"G492\", \"R551\", \"S490\"],\n",
    "    [\"NEW34469\", \"Y990\", \"H898\", \"W673\", \"L967\", \"M829\"],\n",
    "    [\"NEW34462\", \"Y990\", \"H898\", \"W673\", \"L967\", \"M829\"],\n",
    "]\n",
    "\n",
    "_expected = {'gt': 5, 'tp': 4, 'fp': 2, 'fn': 1}\n",
    "\n",
    "#udfs \n",
    "# trim_udf = get_trimmer_udf(trim=0)\n",
    "\n",
    "# Create DataFrames\n",
    "columns = [0, 1, 2, 3, 4, 5]\n",
    "df1 = pd.DataFrame(data1, columns=columns)\n",
    "df2 = pd.DataFrame(data2, columns=columns)\n",
    "\n",
    "df1['origin'] =  1\n",
    "df2['origin'] =  2\n",
    "\n",
    "spark_df1 = spark.createDataFrame(df1)\n",
    "spark_df2 = spark.createDataFrame(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44e9bdf5-7e5a-4156-98b5-b3214722bc79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------------+------------------------------------------------------------------------------------------------+\n",
      "|entity_key          |bucket_id|assigned_df1_ids  |all_column_values                                                                               |\n",
      "+--------------------+---------+------------------+------------------------------------------------------------------------------------------------+\n",
      "|R822W179H017P323F298|ID00009  |[ID00009]         |[{R822, W179, H017, P323, F298}]                                                                |\n",
      "|R746E298Q412L291R232|ID00005  |[ID00005]         |[{N039, E298, Q412, V409, R232}, {R746, E298, Q412, L291, R232}]                                |\n",
      "|N002E396N843V935S719|ID00004  |[ID00004]         |[{N002, E396, N843, I458, S719}, {N002, E396, N843, V935, S719}]                                |\n",
      "|Z011X716M948W967S702|ID00007  |[ID00007]         |[{R449, X716, M948, G667, S702}, {Z011, X716, M948, W967, S702}]                                |\n",
      "|N322K685T432C225W967|NEW30110 |[ID50008, IDTIE00]|[{N322, K685, T442, C225, W967}, {N322, K685, T443, C225, W947}, {N322, K685, T432, C225, W967}]|\n",
      "|J547B222G492R551S490|NEW72348 |[ID50004]         |[{J547, B222, G492, R551, S490}]                                                                |\n",
      "+--------------------+---------+------------------+------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, collect_set, struct\n",
    "\n",
    "cols = ['1', '2', '3', '4', '5']\n",
    "\n",
    "ground_truth_ids = spark_df1.join(spark_df2, on=[\"0\"], how=\"inner\").select(F.col(\"0\").alias(\"df1_id\")).distinct()\n",
    "ground_truth_ids.cache()\n",
    "gt = ground_truth_ids.count()\n",
    "\n",
    "# 1. Create entity keys for df1\n",
    "df1_entities = spark_df1.withColumn(\n",
    "    \"entity_key\", F.concat_ws(\"\", *[F.col(c) for c in cols])\n",
    ").select(F.col(\"0\").alias(\"df1_id\"), \"entity_key\", *cols)\n",
    "\n",
    "# 2. Create entity keys for the bucket (use spark_df2 or concatData)\n",
    "bucket_entities = spark_df2.withColumn(\n",
    "    \"entity_key\", F.concat_ws(\"\", *[F.col(c) for c in cols])\n",
    ").select(\n",
    "    F.col(\"0\").alias(\"bucket_id\"),\n",
    "    \"entity_key\",\n",
    "    *cols\n",
    ")\n",
    "\n",
    "# 3. Cross join and count matches\n",
    "joined = df1_entities.alias(\"a\").crossJoin(bucket_entities.alias(\"b\"))\n",
    "match_exprs = [(F.col(f\"a.{col}\") == F.col(f\"b.{col}\")).cast(\"int\") for col in cols]\n",
    "joined = joined.withColumn(\"match_count\", sum(match_exprs))\n",
    "\n",
    "# 4. Filter pairs with at least 3 matching columns\n",
    "filtered = joined.filter(F.col(\"match_count\") >= 3)\n",
    "\n",
    "# 5. For each df1_id, find the max match_count\n",
    "max_match = filtered.groupBy(\"a.df1_id\").agg(F.max(\"match_count\").alias(\"max_match_count\")).withColumnRenamed(\"df1_id\", \"max_df1_id\")\n",
    "\n",
    "# 6. Join back to get all (df1_id, bucket_id) pairs with the max match_count\n",
    "best_matches = filtered.join(\n",
    "    max_match,\n",
    "    (filtered[\"a.df1_id\"] == max_match[\"max_df1_id\"]) & (filtered[\"match_count\"] == max_match[\"max_match_count\"])\n",
    ").select(\n",
    "    F.col(\"a.df1_id\").alias(\"df1_id\"),\n",
    "    F.col(\"b.bucket_id\").alias(\"bucket_id\"),\n",
    "    F.col(\"b.entity_key\").alias(\"entity_key\"),\n",
    "    F.col(\"match_count\"),\n",
    "    # Use SAME column names for both structs\n",
    "    struct(*[F.col(f\"a.{c}\").alias(c) for c in cols]).alias(\"df1_columns\"),\n",
    "    struct(*[F.col(f\"b.{c}\").alias(c) for c in cols]).alias(\"df2_columns\")\n",
    ")\n",
    "\n",
    "# 7. Group by bucket_id and entity_key, collecting all column variations\n",
    "buckets_with_columns = best_matches.groupBy(\"bucket_id\", \"entity_key\").agg(\n",
    "    F.collect_list(\"df1_id\").alias(\"assigned_df1_ids\"),\n",
    "    collect_set(\"df1_columns\").alias(\"df1_column_values\"),\n",
    "    collect_set(\"df2_columns\").alias(\"df2_column_values\")\n",
    ").withColumn(\n",
    "    \"all_column_values\", \n",
    "    F.array_union(F.col(\"df1_column_values\"), F.col(\"df2_column_values\"))\n",
    ").select(\n",
    "    \"entity_key\",\n",
    "    \"bucket_id\", \n",
    "    \"assigned_df1_ids\",\n",
    "    \"all_column_values\"\n",
    ")\n",
    "\n",
    "buckets_with_columns.cache()\n",
    "buckets_with_columns.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "532a46ba-19c0-459f-8cc6-e331433489b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+\n",
      "|bucket_id|          entity_key|  assigned_df1_ids|\n",
      "+---------+--------------------+------------------+\n",
      "|  ID00009|R822W179H017P323F298|         [ID00009]|\n",
      "|  ID00005|R746E298Q412L291R232|         [ID00005]|\n",
      "|  ID00004|N002E396N843V935S719|         [ID00004]|\n",
      "|  ID00007|Z011X716M948W967S702|         [ID00007]|\n",
      "| NEW30110|N322K685T432C225W967|[ID50008, IDTIE00]|\n",
      "| NEW72348|J547B222G492R551S490|         [ID50004]|\n",
      "+---------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buckets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b6ec6-c3e9-4d12-bda5-9efae26736b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
