{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079a6deb-63e5-47b4-ba57-dbf199b4a0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jellyfish in /opt/conda/lib/python3.10/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18898bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from packages.pyspark.pyspark_pipeline import PySparkPipeline as psp\n",
    "from packages.utils.transformations import MyTransformation as mtf\n",
    "from packages.utils.spark_udfs import get_trimmer_udf, soundex_udf\n",
    "from pyspark.sql.functions import col, array_contains\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import pandas as pd\n",
    "from packages.utils.generate_data_set import SyntheticMatcherDataset as smd\n",
    "\n",
    "\n",
    "trimmer_udf = get_trimmer_udf(trim=1)\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f449aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data for df1\n",
    "data1 = [\n",
    "    [\"ID00005\", \"N039\", \"E298\", \"Q412\", \"V409\", \"R232\"],  # TP1\n",
    "    [\"ID00009\", \"R822\", \"W179\", \"H017\", \"P323\", \"F298\"],  # TP2\n",
    "    [\"ID00007\", \"R449\", \"X716\", \"M948\", \"G667\", \"S702\"],  # TP3\n",
    "    [\"ID00004\", \"N002\", \"E396\", \"N843\", \"I458\", \"S719\"],  # TP4\n",
    "    [\"ID10004\", \"N002\", \"E396\", \"N853\", \"I623\", \"S569\"],  # FN1\n",
    "    [\"ID50004\", \"J547\", \"B222\", \"G492\", \"R551\", \"S490\"],  # FP1\n",
    "    [\"IDTIE00\", \"N322\", \"K685\", \"T442\", \"C225\", \"W967\"],  # FP-tie: this should be skipped\n",
    "    [\"ID50008\", \"N322\", \"K685\", \"T442\", \"C225\", \"W967\"],  # FP2\n",
    "    [\"ID00000\", \"W815\", \"L281\", \"R155\", \"F768\", \"B914\"],\n",
    "    [\"ID00001\", \"C172\", \"B326\", \"X400\", \"M508\", \"O776\"],\n",
    "    [\"ID00002\", \"V683\", \"C265\", \"J127\", \"D589\", \"F482\"],\n",
    "    [\"ID00003\", \"E851\", \"P721\", \"F745\", \"D863\", \"K229\"],\n",
    "    [\"ID00016\", \"T873\", \"D670\", \"U046\", \"Z181\", \"X621\"],\n",
    "    [\"ID00017\", \"F327\", \"G856\", \"E567\", \"O929\", \"Q721\"],\n",
    "    [\"ID00010\", \"O283\", \"T723\", \"Z034\", \"V319\", \"X338\"],\n",
    "]\n",
    "\n",
    "# Data for df2\n",
    "data2 = [\n",
    "    [\"ID00005\", \"R746\", \"E298\", \"Q412\", \"L291\", \"R232\"],  # TP1\n",
    "    [\"ID00009\", \"R822\", \"W179\", \"H017\", \"P323\", \"F298\"],  # TP2\n",
    "    [\"ID00007\", \"Z011\", \"X716\", \"M948\", \"W967\", \"S702\"],  # TP3\n",
    "    [\"ID00004\", \"N002\", \"E396\", \"N843\", \"V935\", \"S719\"],  # TP4\n",
    "    [\"ID10004\", \"N002\", \"E396\", \"N553\", \"I453\", \"S459\"],  # FN1\n",
    "    [\"NEW80187\", \"J547\", \"B222\", \"G492\", \"W673\", \"S490\"],  # FP1\n",
    "    [\"NEW30110\", \"N322\", \"K685\", \"T432\", \"C225\", \"W967\"],  # FP2\n",
    "    [\"NEW72832\", \"F875\", \"Q768\", \"H822\", \"Z154\", \"X678\"],\n",
    "    [\"NEW30110\", \"R560\", \"C434\", \"M687\", \"Q689\", \"Q863\"],\n",
    "    [\"NEW81243\", \"R762\", \"N687\", \"A109\", \"K476\", \"R637\"],\n",
    "    [\"NEW52689\", \"A089\", \"V733\", \"W158\", \"A640\", \"H331\"],\n",
    "    [\"NEW67368\", \"Z079\", \"J617\", \"G878\", \"W111\", \"Q500\"],\n",
    "    [\"NEW72348\", \"J547\", \"B222\", \"G492\", \"R551\", \"S490\"],\n",
    "    [\"NEW34469\", \"Y990\", \"H898\", \"W673\", \"L967\", \"M829\"],\n",
    "    [\"NEW34462\", \"Y990\", \"H898\", \"W673\", \"L967\", \"M829\"],\n",
    "]\n",
    "\n",
    "_expected = {'gt': 5, 'tp': 4, 'fp': 2, 'fn': 1}\n",
    "\n",
    "#udfs \n",
    "# trim_udf = get_trimmer_udf(trim=0)\n",
    "\n",
    "# Create DataFrames\n",
    "columns = [0, 1, 2, 3, 4, 5]\n",
    "df1 = pd.DataFrame(data1, columns=columns)\n",
    "matched_df = pd.DataFrame(data2, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2698a875-d90c-48c1-b497-73bc8ec9e463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "generator = smd(\n",
    "    size=5_000,\n",
    "    ground_truth_ratio=0.25,\n",
    "    datasets_ratio=(1, 1),\n",
    "    true_positive_ratio=0.75,\n",
    "    threshold=3,\n",
    ")\n",
    "_expected = generator.expected\n",
    "df1, matched_df = generator.df1, generator.df2\n",
    "\n",
    "df1.columns = [0,1,2,3,4,5]\n",
    "matched_df.columns = [0,1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aacce899-526e-4fb1-8049-74460baf6703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "transformer = mtf()\n",
    "match_column = 0\n",
    "cols = ['1', '2', '3', '4', '5']\n",
    "\n",
    "df1 =  pd.read_csv(\"data/df1.csv\", usecols=[0,1,2,3,4,5], header=None)[[0,1,2,3,4,5]].sample(frac=0.1, random_state=42)\n",
    "df2 =  pd.read_csv(\"data/df2.csv\", usecols=[0,1,2,3,4,5], header=None)[[0,1,2,3,4,5]].sample(frac=0.1, random_state=42)\n",
    "_expected = {\"gt\": 2531, \"tp\":1775, \"fp\":92, \"fn\":756}\n",
    "\n",
    "transformer.apply_soundex(df1, match_column = 0)\n",
    "transformer.apply_soundex(df2, match_column = 0)\n",
    "\n",
    "matched_df = pd.concat([df2]).drop_duplicates()\n",
    "\n",
    "spark_df1 = spark.createDataFrame(df1)\n",
    "spark_df2 = spark.createDataFrame(df2)\n",
    "\n",
    "for col_name in spark_df1.columns:\n",
    "    if str(col_name) != str(match_column):\n",
    "        spark_df1 = spark_df1.withColumn(str(col_name), trimmer_udf(spark_df1[col_name]))\n",
    "        spark_df2 = spark_df2.withColumn(str(col_name), trimmer_udf(spark_df2[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2371e8e2-59dd-4df8-a3d4-d11a87785351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read CSVs with Spark\n",
    "df1 = spark.read.csv(\"data/df1.csv\", header=None, inferSchema=True).select([\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\"])\n",
    "df2 = spark.read.csv(\"data/df2.csv\", header=None, inferSchema=True).select([\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\"])\n",
    "df3 = spark.read.csv(\"data/df3.csv\", header=None, inferSchema=True).select([\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\"])\n",
    "df4 = spark.read.csv(\"data/df4.csv\", header=None, inferSchema=True).select([\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\"])\n",
    "df5 = spark.read.csv(\"data/df5.csv\", header=None, inferSchema=True).select([\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\"])\n",
    "_expected = {'gt': 25000, 'tp': 24550, 'fp': 12333, 'fn': 450}\n",
    "\n",
    "match_column = \"0\"\n",
    "cols = ['1', '2', '3', '4', '5']\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    df1 = df1.withColumnRenamed(f\"_c{i}\", str(i))\n",
    "    df2 = df2.withColumnRenamed(f\"_c{i}\", str(i))\n",
    "    df3 = df3.withColumnRenamed(f\"_c{i}\", str(i))\n",
    "    df4 = df4.withColumnRenamed(f\"_c{i}\", str(i))\n",
    "    df5 = df5.withColumnRenamed(f\"_c{i}\", str(i))\n",
    "    \n",
    "for col_name in df1.columns:\n",
    "    if col_name != match_column:\n",
    "        df1 = df1.withColumn(str(col_name), soundex_udf(df1[col_name]))\n",
    "        df2 = df2.withColumn(str(col_name), soundex_udf(df2[col_name]))\n",
    "        df3 = df3.withColumn(str(col_name), soundex_udf(df3[col_name]))\n",
    "        df4 = df4.withColumn(str(col_name), soundex_udf(df4[col_name]))\n",
    "        df5 = df5.withColumn(str(col_name), soundex_udf(df5[col_name]))\n",
    "\n",
    "        \n",
    "for col_name in df1.columns:\n",
    "    if col_name != match_column:\n",
    "        df1 = df1.withColumn(str(col_name), trimmer_udf(df1[col_name]))\n",
    "        df2 = df2.withColumn(str(col_name), trimmer_udf(df2[col_name]))\n",
    "        df3 = df3.withColumn(str(col_name), trimmer_udf(df3[col_name]))\n",
    "        df4 = df4.withColumn(str(col_name), trimmer_udf(df4[col_name]))\n",
    "        df5 = df5.withColumn(str(col_name), trimmer_udf(df5[col_name]))        \n",
    "        \n",
    "    \n",
    "# Combine matched_df as before\n",
    "spark_df1 = df1\n",
    "spark_df2 = df2.union(df3).union(df4).union(df5).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2b13432-96f6-490f-af93-d81cc38ad24c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gt': 25000, 'tp': 24550, 'fp': 12333, 'fn': 450}\n",
      "Ground Truth (gt): 25000\n",
      "True Positives (tp): 24921\n",
      "False Positives (fp): 39836\n",
      "False Negative (fn): 79\n",
      "precision: 0.38483870469601744\n",
      "Recall: 0.99684\n"
     ]
    }
   ],
   "source": [
    "ground_truth_ids = spark_df1.join(spark_df2, on=[\"0\"], how=\"inner\").select(F.col(\"0\").alias(\"df1_id\")).distinct()\n",
    "gt = ground_truth_ids.count()\n",
    "\n",
    "# 1. Create entity keys for df1 and matched_df\n",
    "df1_entities = spark_df1.withColumn(\n",
    "    \"entity_key\", F.concat_ws(\"\", *[F.col(c) for c in cols])\n",
    ").select(F.col(\"0\").alias(\"df1_id\"), \"entity_key\", *cols)\n",
    "\n",
    "matched_entities = spark_df2.withColumn(\n",
    "    \"entity_key\", F.concat_ws(\"\", *[F.col(c) for c in cols])\n",
    ").select(F.col(\"0\").alias(\"matched_id\"), \"entity_key\", *cols)\n",
    "\n",
    "# 2. Cross join and count matches\n",
    "joined = df1_entities.alias(\"a\").crossJoin(matched_entities.alias(\"b\"))\n",
    "match_exprs = [(F.col(f\"a.{col}\") == F.col(f\"b.{col}\")).cast(\"int\") for col in cols]\n",
    "joined = joined.withColumn(\"match_count\", sum(match_exprs))\n",
    "\n",
    "# 3. Filter pairs with at least 3 matching columns\n",
    "filtered = joined.filter(F.col(\"match_count\") >= 3)\n",
    "\n",
    "# 4. For each df1_id, find the max match_count\n",
    "max_match = filtered.groupBy(\"a.df1_id\").agg(F.max(\"match_count\").alias(\"max_match_count\")).withColumnRenamed(\"df1_id\", \"max_df1_id\")\n",
    "\n",
    "# 5. Join back to get all (df1_id, matched_id) pairs with the max match_count\n",
    "best_matches = filtered.join(\n",
    "    max_match,\n",
    "    (filtered[\"a.df1_id\"] == max_match[\"max_df1_id\"]) & (filtered[\"match_count\"] == max_match[\"max_match_count\"])\n",
    ").select(\n",
    "    F.col(\"a.df1_id\").alias(\"df1_id\"),\n",
    "    F.col(\"b.matched_id\").alias(\"matched_id\"),\n",
    "    F.col(\"match_count\")\n",
    ")\n",
    "\n",
    "# 6. Group by matched_id (bucket) to collect all df1_ids assigned to it\n",
    "buckets = best_matches.groupBy(\"matched_id\").agg(\n",
    "    F.collect_list(\"df1_id\").alias(\"assigned_df1_ids\")\n",
    ")\n",
    "\n",
    "# True Positives\n",
    "tp_arr = buckets \\\n",
    "    .filter(array_contains(col(\"assigned_df1_ids\"), col(\"matched_id\"))) \\\n",
    "    .join(ground_truth_ids, buckets.matched_id == ground_truth_ids.df1_id, how=\"inner\")\n",
    "\n",
    "tp = tp_arr.count()\n",
    "\n",
    "fp = buckets.filter(~array_contains(col(\"assigned_df1_ids\"), col(\"matched_id\"))).count()\n",
    "fn = gt - tp\n",
    "\n",
    "precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "print(_expected)\n",
    "print(f\"Ground Truth (gt): {gt}\")\n",
    "print(f\"True Positives (tp): {tp}\")\n",
    "print(f\"False Positives (fp): {fp}\")\n",
    "print(f\"False Negative (fn): {fn}\")\n",
    "print(f\"precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efc660d3-6c2c-4ea4-803f-1248d3fe22b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+---+---+\n",
      "|       0|  1|  2|  3|  4|  5|\n",
      "+--------+---+---+---+---+---+\n",
      "|AA223369|B65|I21|E10|423|B64|\n",
      "| AA41454|S53|W45|K53|236|B64|\n",
      "|AA144378|C16|B63|H36|415|G65|\n",
      "| AA25364|K65|E62|W32|355|E45|\n",
      "|  AA8583|C63|L60|R00|236|B64|\n",
      "| AA24201|J52|L54|W12|163|G65|\n",
      "| AA57788|B63|D54|W45|335|F31|\n",
      "| AA45000|T65|J50|L00|252|B64|\n",
      "|AA205928|P20|J25|C41|216|H61|\n",
      "|AA193095|P34|Y20|P52|514|M15|\n",
      "|AA157869|B65|G53|F32|161|M24|\n",
      "|AA182660|K53|H40|R12|316|B64|\n",
      "|AA186589|R12|J16|C43|355|B64|\n",
      "|AA138813|H15|A42|F00|155|G65|\n",
      "|AA145313|S00|A35|L00|116|E45|\n",
      "| AA72910|W25|T53|A45|526|W45|\n",
      "|AA100470|W32|L60|E41|362|G12|\n",
      "|AA105323|C45|B65|P40|233|S52|\n",
      "|AA208694|F52|W45|J52|215|B64|\n",
      "|AA215809|G63|D15|L00|614|S52|\n",
      "+--------+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3386991-2455-467b-ac39-c2a85f115b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'gt': 2531, 'tp': 1775, 'fp': 92, 'fn': 756}\n",
    "Ground Truth (gt): 25000\n",
    "True Positives (tp): 24550\n",
    "False Positives (fp): 12333\n",
    "False Negative (fn): 450\n",
    "precision: 0.6656183065368869\n",
    "Recall: 0.982"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
