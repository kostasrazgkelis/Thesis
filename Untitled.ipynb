{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0519805f-0560-4f4c-9de6-703ac57073c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import pandas as pd\n",
    "from packages.utils.transformations import MyTransformation as mtf\n",
    "from pyspark.sql.functions import col, array_contains\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"example2\").getOrCreate()\n",
    "\n",
    "# Read and rename columns as before\n",
    "def read_with_origin(path, origin):\n",
    "    df = spark.read.csv(path, header=None, inferSchema=True).select([\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\"])\n",
    "    for i in range(6):\n",
    "        df = df.withColumnRenamed(f\"_c{i}\", str(i))\n",
    "    df = df.withColumn(\"origin\", lit(origin))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8113b0-23c8-4329-9f82-466506cfb5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "transformer = mtf()\n",
    "match_column = 0\n",
    "cols = ['1', '2', '3', '4', '5']\n",
    "\n",
    "df1 =  pd.read_csv(\"data/df1.csv\", usecols=[0,1,2,3,4,5], header=None)[[0,1,2,3,4,5]].sample(frac=0.1, random_state=42)\n",
    "df2 =  pd.read_csv(\"data/df2.csv\", usecols=[0,1,2,3,4,5], header=None)[[0,1,2,3,4,5]].sample(frac=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "_expected = {\"gt\": 2531, \"tp\":1775, \"fp\":92, \"fn\":756}\n",
    "\n",
    "transformer.apply_soundex(df1, match_column = 0)\n",
    "transformer.apply_soundex(df2, match_column = 0)\n",
    "\n",
    "df1['origin'] =  1\n",
    "df2['origin'] =  2\n",
    "\n",
    "spark_df1 = spark.createDataFrame(df1)\n",
    "spark_df2 = spark.createDataFrame(df2)\n",
    "\n",
    "concatData = spark_df1.union(spark_df2)\n",
    "# for col_name in spark_df1.columns:\n",
    "#     if str(col_name) != str(match_column):\n",
    "#         spark_df1 = spark_df1.withColumn(str(col_name), trimmer_udf(spark_df1[col_name]))\n",
    "#         spark_df2 = spark_df2.withColumn(str(col_name), trimmer_udf(spark_df2[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7e261-19bf-447a-b42d-a4c60a4f4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = read_with_origin(\"data/df1.csv\", \"df1\")\n",
    "df2 = read_with_origin(\"data/df2.csv\", \"df2\")\n",
    "df3 = read_with_origin(\"data/df3.csv\", \"df3\")\n",
    "df4 = read_with_origin(\"data/df4.csv\", \"df4\")\n",
    "df5 = read_with_origin(\"data/df5.csv\", \"df5\")\n",
    "\n",
    "for col_name in df1.columns:\n",
    "    if col_name != match_column:\n",
    "        df1 = df1.withColumn(str(col_name), soundex_udf(df1[col_name]))\n",
    "        df2 = df2.withColumn(str(col_name), soundex_udf(df2[col_name]))\n",
    "        df3 = df3.withColumn(str(col_name), soundex_udf(df3[col_name]))\n",
    "        df4 = df4.withColumn(str(col_name), soundex_udf(df4[col_name]))\n",
    "        df5 = df5.withColumn(str(col_name), soundex_udf(df5[col_name]))\n",
    "        \n",
    "# Concatenate and drop duplicates\n",
    "concatData = df1.union(df2).union(df3).union(df4).union(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cefdc0c-fdd7-4f6a-a3d7-e25750464bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|bucket_id|    assigned_df1_ids|\n",
      "+---------+--------------------+\n",
      "| AA100187|          [AA100187]|\n",
      "| AA100252|          [AA100252]|\n",
      "| AA100290|          [AA100290]|\n",
      "| AA100327|          [AA100327]|\n",
      "| AA100360| [AA81619, AA100360]|\n",
      "| AA100381|          [AA100381]|\n",
      "| AA100422|          [AA100422]|\n",
      "|  AA10048|[AA97299, AA11609...|\n",
      "| AA100508|          [AA100508]|\n",
      "| AA100514|          [AA100514]|\n",
      "| AA100613|[AA192247, AA100613]|\n",
      "| AA100621|          [AA100621]|\n",
      "| AA100654|          [AA100654]|\n",
      "| AA100727|[AA100727, AA196019]|\n",
      "| AA100824|          [AA100824]|\n",
      "| AA100835|          [AA100835]|\n",
      "| AA100888|          [AA100888]|\n",
      "| AA100904|          [AA100904]|\n",
      "| AA100932|          [AA100932]|\n",
      "| AA100966|          [AA100966]|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['1', '2', '3', '4', '5']\n",
    "\n",
    "ground_truth_ids = spark_df1.join(spark_df2, on=[\"0\"], how=\"inner\").select(F.col(\"0\").alias(\"df1_id\")).distinct()\n",
    "gt = ground_truth_ids.count()\n",
    "\n",
    "# 1. Create entity keys for df1\n",
    "df1_entities = spark_df1.withColumn(\n",
    "    \"entity_key\", F.concat_ws(\"\", *[F.col(c) for c in cols])\n",
    ").select(F.col(\"0\").alias(\"df1_id\"), \"entity_key\", *cols)\n",
    "\n",
    "# 2. Create entity keys for the bucket (use spark_df2 or concatData)\n",
    "bucket_entities = spark_df2.withColumn(\n",
    "    \"entity_key\", F.concat_ws(\"\", *[F.col(c) for c in cols])\n",
    ").select(\n",
    "    F.col(\"0\").alias(\"bucket_id\"),\n",
    "    \"entity_key\",\n",
    "    *cols\n",
    ")\n",
    "\n",
    "# 3. Cross join and count matches\n",
    "joined = df1_entities.alias(\"a\").crossJoin(bucket_entities.alias(\"b\"))\n",
    "match_exprs = [(F.col(f\"a.{col}\") == F.col(f\"b.{col}\")).cast(\"int\") for col in cols]\n",
    "joined = joined.withColumn(\"match_count\", sum(match_exprs))\n",
    "\n",
    "# 4. Filter pairs with at least 3 matching columns\n",
    "filtered = joined.filter(F.col(\"match_count\") >= 3)\n",
    "\n",
    "# 5. For each df1_id, find the max match_count\n",
    "max_match = filtered.groupBy(\"a.df1_id\").agg(F.max(\"match_count\").alias(\"max_match_count\")).withColumnRenamed(\"df1_id\", \"max_df1_id\")\n",
    "\n",
    "# 6. Join back to get all (df1_id, bucket_id) pairs with the max match_count\n",
    "best_matches = filtered.join(\n",
    "    max_match,\n",
    "    (filtered[\"a.df1_id\"] == max_match[\"max_df1_id\"]) & (filtered[\"match_count\"] == max_match[\"max_match_count\"])\n",
    ").select(\n",
    "    F.col(\"a.df1_id\").alias(\"df1_id\"),\n",
    "    F.col(\"b.bucket_id\").alias(\"bucket_id\"),\n",
    "    F.col(\"match_count\")\n",
    ")\n",
    "\n",
    "# 7. Group by bucket_id to collect all df1_ids assigned to it\n",
    "buckets = best_matches.groupBy(\"bucket_id\").agg(\n",
    "    F.collect_list(\"df1_id\").alias(\"assigned_df1_ids\")\n",
    ")\n",
    "\n",
    "buckets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27ad3647-d0d0-427f-bb5b-eeb8aedd1894",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'matched_id' does not exist. Did you mean one of the following? [bucket_id, assigned_df1_ids];\n'Filter array_contains(assigned_df1_ids#442, 'matched_id)\n+- Aggregate [bucket_id#434], [bucket_id#434, collect_list(df1_id#433, 0, 0) AS assigned_df1_ids#442]\n   +- Project [df1_id#308 AS df1_id#433, bucket_id#325 AS bucket_id#434, match_count#347]\n      +- Join Inner, ((df1_id#308 = max_df1_id#382) AND (match_count#347 = max_match_count#379))\n         :- Filter (match_count#347 >= 3)\n         :  +- Project [df1_id#308, entity_key#299, 1#154, 2#155, 3#156, 4#157, 5#158, bucket_id#325, entity_key#316, 1#168, 2#169, 3#170, 4#171, 5#172, (((((cast((1#154 = 1#168) as int) + 0) + cast((2#155 = 2#169) as int)) + cast((3#156 = 3#170) as int)) + cast((4#157 = 4#171) as int)) + cast((5#158 = 5#172) as int)) AS match_count#347]\n         :     +- Join Cross\n         :        :- SubqueryAlias a\n         :        :  +- Project [0#153 AS df1_id#308, entity_key#299, 1#154, 2#155, 3#156, 4#157, 5#158]\n         :        :     +- Project [0#153, 1#154, 2#155, 3#156, 4#157, 5#158, origin#159L, concat_ws(, 1#154, 2#155, 3#156, 4#157, 5#158) AS entity_key#299]\n         :        :        +- LogicalRDD [0#153, 1#154, 2#155, 3#156, 4#157, 5#158, origin#159L], false\n         :        +- SubqueryAlias b\n         :           +- Project [0#167 AS bucket_id#325, entity_key#316, 1#168, 2#169, 3#170, 4#171, 5#172]\n         :              +- Project [0#167, 1#168, 2#169, 3#170, 4#171, 5#172, origin#173L, concat_ws(, 1#168, 2#169, 3#170, 4#171, 5#172) AS entity_key#316]\n         :                 +- LogicalRDD [0#167, 1#168, 2#169, 3#170, 4#171, 5#172, origin#173L], false\n         +- Project [df1_id#308 AS max_df1_id#382, max_match_count#379]\n            +- Aggregate [df1_id#308], [df1_id#308, max(match_count#347) AS max_match_count#379]\n               +- Filter (match_count#347 >= 3)\n                  +- Project [df1_id#308, entity_key#299, 1#386, 2#387, 3#388, 4#389, 5#390, bucket_id#325, entity_key#316, 1#393, 2#394, 3#395, 4#396, 5#397, (((((cast((1#386 = 1#393) as int) + 0) + cast((2#387 = 2#394) as int)) + cast((3#388 = 3#395) as int)) + cast((4#389 = 4#396) as int)) + cast((5#390 = 5#397) as int)) AS match_count#347]\n                     +- Join Cross\n                        :- SubqueryAlias a\n                        :  +- Project [0#385 AS df1_id#308, entity_key#299, 1#386, 2#387, 3#388, 4#389, 5#390]\n                        :     +- Project [0#385, 1#386, 2#387, 3#388, 4#389, 5#390, origin#391L, concat_ws(, 1#386, 2#387, 3#388, 4#389, 5#390) AS entity_key#299]\n                        :        +- LogicalRDD [0#385, 1#386, 2#387, 3#388, 4#389, 5#390, origin#391L], false\n                        +- SubqueryAlias b\n                           +- Project [0#392 AS bucket_id#325, entity_key#316, 1#393, 2#394, 3#395, 4#396, 5#397]\n                              +- Project [0#392, 1#393, 2#394, 3#395, 4#396, 5#397, origin#398L, concat_ws(, 1#393, 2#394, 3#395, 4#396, 5#397) AS entity_key#316]\n                                 +- LogicalRDD [0#392, 1#393, 2#394, 3#395, 4#396, 5#397, origin#398L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# True Positives\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tp_arr \u001b[38;5;241m=\u001b[39m \u001b[43mbuckets\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_contains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massigned_df1_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatched_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(ground_truth_ids, buckets\u001b[38;5;241m.\u001b[39mmatched_id \u001b[38;5;241m==\u001b[39m ground_truth_ids\u001b[38;5;241m.\u001b[39mdf1_id, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m tp \u001b[38;5;241m=\u001b[39m tp_arr\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m      8\u001b[0m fp \u001b[38;5;241m=\u001b[39m buckets\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;241m~\u001b[39marray_contains(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massigned_df1_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatched_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:2079\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   2077\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 2079\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition should be string or Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'matched_id' does not exist. Did you mean one of the following? [bucket_id, assigned_df1_ids];\n'Filter array_contains(assigned_df1_ids#442, 'matched_id)\n+- Aggregate [bucket_id#434], [bucket_id#434, collect_list(df1_id#433, 0, 0) AS assigned_df1_ids#442]\n   +- Project [df1_id#308 AS df1_id#433, bucket_id#325 AS bucket_id#434, match_count#347]\n      +- Join Inner, ((df1_id#308 = max_df1_id#382) AND (match_count#347 = max_match_count#379))\n         :- Filter (match_count#347 >= 3)\n         :  +- Project [df1_id#308, entity_key#299, 1#154, 2#155, 3#156, 4#157, 5#158, bucket_id#325, entity_key#316, 1#168, 2#169, 3#170, 4#171, 5#172, (((((cast((1#154 = 1#168) as int) + 0) + cast((2#155 = 2#169) as int)) + cast((3#156 = 3#170) as int)) + cast((4#157 = 4#171) as int)) + cast((5#158 = 5#172) as int)) AS match_count#347]\n         :     +- Join Cross\n         :        :- SubqueryAlias a\n         :        :  +- Project [0#153 AS df1_id#308, entity_key#299, 1#154, 2#155, 3#156, 4#157, 5#158]\n         :        :     +- Project [0#153, 1#154, 2#155, 3#156, 4#157, 5#158, origin#159L, concat_ws(, 1#154, 2#155, 3#156, 4#157, 5#158) AS entity_key#299]\n         :        :        +- LogicalRDD [0#153, 1#154, 2#155, 3#156, 4#157, 5#158, origin#159L], false\n         :        +- SubqueryAlias b\n         :           +- Project [0#167 AS bucket_id#325, entity_key#316, 1#168, 2#169, 3#170, 4#171, 5#172]\n         :              +- Project [0#167, 1#168, 2#169, 3#170, 4#171, 5#172, origin#173L, concat_ws(, 1#168, 2#169, 3#170, 4#171, 5#172) AS entity_key#316]\n         :                 +- LogicalRDD [0#167, 1#168, 2#169, 3#170, 4#171, 5#172, origin#173L], false\n         +- Project [df1_id#308 AS max_df1_id#382, max_match_count#379]\n            +- Aggregate [df1_id#308], [df1_id#308, max(match_count#347) AS max_match_count#379]\n               +- Filter (match_count#347 >= 3)\n                  +- Project [df1_id#308, entity_key#299, 1#386, 2#387, 3#388, 4#389, 5#390, bucket_id#325, entity_key#316, 1#393, 2#394, 3#395, 4#396, 5#397, (((((cast((1#386 = 1#393) as int) + 0) + cast((2#387 = 2#394) as int)) + cast((3#388 = 3#395) as int)) + cast((4#389 = 4#396) as int)) + cast((5#390 = 5#397) as int)) AS match_count#347]\n                     +- Join Cross\n                        :- SubqueryAlias a\n                        :  +- Project [0#385 AS df1_id#308, entity_key#299, 1#386, 2#387, 3#388, 4#389, 5#390]\n                        :     +- Project [0#385, 1#386, 2#387, 3#388, 4#389, 5#390, origin#391L, concat_ws(, 1#386, 2#387, 3#388, 4#389, 5#390) AS entity_key#299]\n                        :        +- LogicalRDD [0#385, 1#386, 2#387, 3#388, 4#389, 5#390, origin#391L], false\n                        +- SubqueryAlias b\n                           +- Project [0#392 AS bucket_id#325, entity_key#316, 1#393, 2#394, 3#395, 4#396, 5#397]\n                              +- Project [0#392, 1#393, 2#394, 3#395, 4#396, 5#397, origin#398L, concat_ws(, 1#393, 2#394, 3#395, 4#396, 5#397) AS entity_key#316]\n                                 +- LogicalRDD [0#392, 1#393, 2#394, 3#395, 4#396, 5#397, origin#398L], false\n"
     ]
    }
   ],
   "source": [
    "# True Positives\n",
    "tp_arr = buckets \\\n",
    "    .filter(array_contains(col(\"assigned_df1_ids\"), col(\"matched_id\"))) \\\n",
    "    .join(ground_truth_ids, buckets.matched_id == ground_truth_ids.df1_id, how=\"inner\")\n",
    "\n",
    "tp = tp_arr.count()\n",
    "\n",
    "fp = buckets.filter(~array_contains(col(\"assigned_df1_ids\"), col(\"matched_id\"))).count()\n",
    "fn = gt - tp\n",
    "\n",
    "precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "print(_expected)\n",
    "print(f\"Ground Truth (gt): {gt}\")\n",
    "print(f\"True Positives (tp): {tp}\")\n",
    "print(f\"False Positives (fp): {fp}\")\n",
    "print(f\"False Negative (fn): {fn}\")\n",
    "print(f\"precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0e73235-51ea-458d-be60-f5220a830752",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|bucket_id|    assigned_df1_ids|\n",
      "+---------+--------------------+\n",
      "| AA100187|          [AA100187]|\n",
      "| AA100252|          [AA100252]|\n",
      "| AA100290|          [AA100290]|\n",
      "| AA100327|          [AA100327]|\n",
      "| AA100360| [AA81619, AA100360]|\n",
      "| AA100381|          [AA100381]|\n",
      "| AA100422|          [AA100422]|\n",
      "|  AA10048|[AA97299, AA11609...|\n",
      "| AA100508|          [AA100508]|\n",
      "| AA100514|          [AA100514]|\n",
      "| AA100613|[AA192247, AA100613]|\n",
      "| AA100621|          [AA100621]|\n",
      "| AA100654|          [AA100654]|\n",
      "| AA100727|[AA100727, AA196019]|\n",
      "| AA100824|          [AA100824]|\n",
      "| AA100835|          [AA100835]|\n",
      "| AA100888|          [AA100888]|\n",
      "| AA100904|          [AA100904]|\n",
      "| AA100932|          [AA100932]|\n",
      "| AA100966|          [AA100966]|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buckets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82f5ab-cd71-426f-b264-53f8135ade37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
